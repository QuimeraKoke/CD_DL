### **Ejemplo Inception: Uso de Convoluciones 1x1 para Reducción de Dimensionalidad**

**Objetivo del Ejemplo:**
Ver en acción cómo una capa de convolución de 1x1, también conocida como "capa de cuello de botella" (bottleneck), reduce drásticamente la dimensionalidad de un tensor (específicamente, el número de canales o filtros) sin alterar sus dimensiones espaciales (alto y ancho).

---

### **1. Preparación del Entorno y Creación de un Tensor de Entrada**

Primero, importamos TensorFlow. Luego, crearemos un tensor de entrada "dummy" que simule la salida de una capa anterior dentro de una red profunda. Usaremos dimensiones que son comunes en arquitecturas como Inception, por ejemplo, una gran cantidad de canales (profundidad).

```python
import tensorflow as tf
from tensorflow.keras import layers

# Dimensiones de nuestro tensor de entrada simulado
# (batch_size, alto, ancho, canales)
batch_size = 1
height = 28
width = 28
input_channels = 192 # Un número de canales alto, como podría ocurrir en una red profunda

# Creamos el tensor de entrada con valores aleatorios
input_tensor = tf.random.normal((batch_size, height, width, input_channels))

print(f"Dimensiones del Tensor de Entrada: {input_tensor.shape}")
```

### **2. Aplicando la Convolución 1x1 (El Cuello de Botella)**

Ahora, definiremos una capa `Conv2D` con un `kernel_size=(1, 1)`. El objetivo es reducir los 192 canales de entrada a un número mucho menor, por ejemplo, 32. Esta operación es el núcleo de la optimización en los módulos Inception.

```python
# Número de canales que queremos en la salida
output_channels = 32

# Definimos la capa de convolución 1x1
bottleneck_conv_1x1 = layers.Conv2D(
    filters=output_channels,
    kernel_size=(1, 1), # La clave está aquí
    padding="same",
    activation="relu"
)

# Aplicamos la capa a nuestro tensor de entrada
output_tensor = bottleneck_conv_1x1(input_tensor)

print(f"Dimensiones del Tensor de Salida: {output_tensor.shape}")
```

### **3. Análisis del Resultado**

Observa las dimensiones impresas:

* **Tensor de Entrada:** `(1, 28, 28, 192)`
* **Tensor de Salida:** `(1, 28, 28, 32)`

Como puedes ver, las dimensiones espaciales (`alto=28`, `ancho=28`) se mantuvieron idénticas. Sin embargo, la profundidad (el número de canales) se redujo drásticamente de **192 a 32**.

**Conclusión Clave:** 💡

Hemos realizado una "compresión" de la información a través de los canales con un costo computacional muy bajo. Ahora, si quisiéramos aplicar una convolución costosa de 3x3 o 5x5, la haríamos sobre este `output_tensor` de 32 canales en lugar del `input_tensor` original de 192 canales, **ahorrando una enorme cantidad de cálculos**.

Este simple "truco" es lo que permite que la compleja idea de procesamiento multi-escala de Inception sea eficiente y práctica.