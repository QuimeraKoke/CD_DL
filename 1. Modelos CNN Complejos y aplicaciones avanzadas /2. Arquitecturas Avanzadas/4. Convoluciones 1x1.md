### **Ejemplo Inception: Uso de Convoluciones 1x1 para Reducci贸n de Dimensionalidad**

**Objetivo del Ejemplo:**
Ver en acci贸n c贸mo una capa de convoluci贸n de 1x1, tambi茅n conocida como "capa de cuello de botella" (bottleneck), reduce dr谩sticamente la dimensionalidad de un tensor (espec铆ficamente, el n煤mero de canales o filtros) sin alterar sus dimensiones espaciales (alto y ancho).

---

### **1. Preparaci贸n del Entorno y Creaci贸n de un Tensor de Entrada**

Primero, importamos TensorFlow. Luego, crearemos un tensor de entrada "dummy" que simule la salida de una capa anterior dentro de una red profunda. Usaremos dimensiones que son comunes en arquitecturas como Inception, por ejemplo, una gran cantidad de canales (profundidad).

```python
import tensorflow as tf
from tensorflow.keras import layers

# Dimensiones de nuestro tensor de entrada simulado
# (batch_size, alto, ancho, canales)
batch_size = 1
height = 28
width = 28
input_channels = 192 # Un n煤mero de canales alto, como podr铆a ocurrir en una red profunda

# Creamos el tensor de entrada con valores aleatorios
input_tensor = tf.random.normal((batch_size, height, width, input_channels))

print(f"Dimensiones del Tensor de Entrada: {input_tensor.shape}")
```

### **2. Aplicando la Convoluci贸n 1x1 (El Cuello de Botella)**

Ahora, definiremos una capa `Conv2D` con un `kernel_size=(1, 1)`. El objetivo es reducir los 192 canales de entrada a un n煤mero mucho menor, por ejemplo, 32. Esta operaci贸n es el n煤cleo de la optimizaci贸n en los m贸dulos Inception.

```python
# N煤mero de canales que queremos en la salida
output_channels = 32

# Definimos la capa de convoluci贸n 1x1
bottleneck_conv_1x1 = layers.Conv2D(
    filters=output_channels,
    kernel_size=(1, 1), # La clave est谩 aqu铆
    padding="same",
    activation="relu"
)

# Aplicamos la capa a nuestro tensor de entrada
output_tensor = bottleneck_conv_1x1(input_tensor)

print(f"Dimensiones del Tensor de Salida: {output_tensor.shape}")
```

### **3. An谩lisis del Resultado**

Observa las dimensiones impresas:

* **Tensor de Entrada:** `(1, 28, 28, 192)`
* **Tensor de Salida:** `(1, 28, 28, 32)`

Como puedes ver, las dimensiones espaciales (`alto=28`, `ancho=28`) se mantuvieron id茅nticas. Sin embargo, la profundidad (el n煤mero de canales) se redujo dr谩sticamente de **192 a 32**.

**Conclusi贸n Clave:** 

Hemos realizado una "compresi贸n" de la informaci贸n a trav茅s de los canales con un costo computacional muy bajo. Ahora, si quisi茅ramos aplicar una convoluci贸n costosa de 3x3 o 5x5, la har铆amos sobre este `output_tensor` de 32 canales en lugar del `input_tensor` original de 192 canales, **ahorrando una enorme cantidad de c谩lculos**.

Este simple "truco" es lo que permite que la compleja idea de procesamiento multi-escala de Inception sea eficiente y pr谩ctica.