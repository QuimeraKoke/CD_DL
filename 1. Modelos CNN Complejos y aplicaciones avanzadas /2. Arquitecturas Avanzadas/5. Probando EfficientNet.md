### **Ejemplo EfficientNet: Ilustraci贸n del Escalado Simple vs. Compuesto**

**(Diagrama conceptual, sin c贸digo complejo)**

**Objetivo del Ejemplo:**
Comparar num茅ricamente c贸mo crecen las dimensiones de una red (profundidad, ancho, resoluci贸n) cuando se usa un "escalado simple" (modificando una sola dimensi贸n) frente a un "escalado compuesto". El objetivo es entender *por qu茅* el enfoque de EfficientNet es m谩s equilibrado.

---

### **1. Definici贸n de Nuestra Red Base ("BaseNet") y Coeficientes de Escalado**

Imaginemos que tenemos una red neuronal base, "BaseNet", con las siguientes caracter铆sticas. Tambi茅n definiremos los coeficientes de escalado que EfficientNet encontr贸 ($\alpha, \beta, \gamma$).

```python
# --- Par谩metros de nuestra red base ---
base_depth = 18      # ej. 18 capas
base_width = 32      # ej. 32 filtros en una capa de referencia
base_resolution = 128  # ej. im谩genes de entrada de 128x128

print("--- Dimensiones de BaseNet ---")
print(f"Profundidad: {base_depth}")
print(f"Ancho (Filtros): {base_width}")
print(f"Resoluci贸n: {base_resolution}x{base_resolution}")

# --- Coeficientes de Escalado de EfficientNet ---
# Estos son los valores aproximados del paper para escalar la red
alpha = 1.2  # Coeficiente para la profundidad
beta = 1.1   # Coeficiente para el ancho
gamma = 1.15 # Coeficiente para la resoluci贸n

# --- Coeficiente Compuesto (nuestro presupuesto de recursos) ---
# Elegimos un phi para escalar. Un phi=3 significa que queremos una red
# significativamente m谩s grande que la base.
phi = 3.0
```

---
### **2. Estrategia de Escalado Simple**

Ahora, veamos qu茅 pasa si usamos todos nuestros "recursos" ($\approx 2^\phi$) para escalar una sola dimensi贸n.

#### **2.1. Escalado Simple: Solo Profundidad**
Hacemos la red mucho m谩s profunda, pero mantenemos el ancho y la resoluci贸n originales.

```python
# Aumentamos la profundidad usando alpha y phi
scaled_depth_simple = base_depth * (alpha ** phi)

print("\n--- Escalado Simple (Solo Profundidad) ---")
print(f"Nueva Profundidad: {round(scaled_depth_simple)}")
print(f"Ancho (sin cambios): {base_width}")
print(f"Resoluci贸n (sin cambios): {base_resolution}")
print(">> Resultado: Una red muy profunda y 'delgada'. Podr铆a tener problemas de gradientes y no ser capaz de capturar caracter铆sticas complejas en cada capa.")
```

#### **2.2. Escalado Simple: Solo Ancho**
Hacemos la red mucho m谩s ancha, pero con su profundidad y resoluci贸n originales.

```python
# Aumentamos el ancho usando beta y phi
scaled_width_simple = base_width * (beta ** phi)

print("\n--- Escalado Simple (Solo Ancho) ---")
print(f"Profundidad (sin cambios): {base_depth}")
print(f"Nuevo Ancho (Filtros): {round(scaled_width_simple)}")
print(f"Resoluci贸n (sin cambios): {base_resolution}")
print(">> Resultado: Una red muy ancha pero 'superficial'. Puede capturar caracter铆sticas muy ricas en una capa, pero le falta la jerarqu铆a para aprender conceptos abstractos.")
```

#### **2.3. Escalado Simple: Solo Resoluci贸n**
Usamos im谩genes mucho m谩s grandes, pero con la arquitectura original.

```python
# Aumentamos la resoluci贸n usando gamma y phi
scaled_resolution_simple = base_resolution * (gamma ** phi)

print("\n--- Escalado Simple (Solo Resoluci贸n) ---")
print(f"Profundidad (sin cambios): {base_depth}")
print(f"Ancho (sin cambios): {base_width}")
print(f"Nueva Resoluci贸n: {round(scaled_resolution_simple)}x{round(scaled_resolution_simple)}")
print(">> Resultado: El modelo recibe m谩s detalles, pero su arquitectura 'peque帽a' (poca profundidad y ancho) puede no tener la capacidad suficiente para procesarlos eficazmente.")
```

---
### **3. Estrategia de Escalado Compuesto (El Enfoque de EfficientNet)**

Ahora, aplicaremos el m茅todo de EfficientNet, distribuyendo los recursos de escalado ($\phi$) entre las tres dimensiones de manera equilibrada usando $\alpha, \beta, \gamma$.

```python
# Aplicamos las f贸rmulas de escalado compuesto
scaled_depth_compound = base_depth * (alpha ** phi)
scaled_width_compound = base_width * (beta ** phi)
scaled_resolution_compound = base_resolution * (gamma ** phi)

print("\n--- Escalado Compuesto (EfficientNet) ---")
print(f"Nueva Profundidad: {round(scaled_depth_compound)}")
print(f"Nuevo Ancho (Filtros): {round(scaled_width_compound)}")
print(f"Nueva Resoluci贸n: {round(scaled_resolution_compound)}x{round(scaled_resolution_compound)}")
print(">> Resultado: Una red que crece en todas sus dimensiones de manera arm贸nica. Es m谩s profunda, m谩s ancha y procesa im谩genes de mayor resoluci贸n, todo en proporci贸n.")
```

---
### **4. Comparaci贸n y Conclusi贸n**

Veamos los resultados en una tabla para apreciar la diferencia conceptual.

| Estrategia de Escalado | Profundidad | Ancho (Filtros) | Resoluci贸n | Observaci贸n |
| :----------------------- | :---------: | :-------------: | :--------: | :---------------------------------- |
| **BaseNet** | 18          | 32              | 128x128    | Modelo inicial.                     |
| **Simple (Solo Profundidad)** | **~31** | 32              | 128x128    | Desequilibrado: profundo y delgado. |
| **Simple (Solo Ancho)** | 18          | **~43** | 128x128    | Desequilibrado: ancho y superficial.|
| **Simple (Solo Resoluci贸n)** | 18          | 32              | **~195x195** | Desequilibrado: modelo peque帽o para im谩genes grandes. |
| **Compuesto (EfficientNet)** | **~31** | **~43** | **~195x195** | **Equilibrado:** Todas las dimensiones crecen juntas. |


**Conclusi贸n Clave:** 

Este ejemplo num茅rico, sin necesidad de entrenar un modelo, ilustra la filosof铆a de EfficientNet. Mientras que el escalado simple crea arquitecturas "extremas" y desequilibradas, el **escalado compuesto produce un crecimiento arm贸nico**. Esta armon铆a entre la capacidad de la red (profundidad y ancho) y la informaci贸n que recibe (resoluci贸n) es la raz贸n por la que EfficientNet logra una eficiencia y precisi贸n tan notables.