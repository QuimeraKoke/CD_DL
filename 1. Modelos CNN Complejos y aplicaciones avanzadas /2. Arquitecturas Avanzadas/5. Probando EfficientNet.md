### **Ejemplo EfficientNet: Ilustración del Escalado Simple vs. Compuesto**

**(Diagrama conceptual, sin código complejo)**

**Objetivo del Ejemplo:**
Comparar numéricamente cómo crecen las dimensiones de una red (profundidad, ancho, resolución) cuando se usa un "escalado simple" (modificando una sola dimensión) frente a un "escalado compuesto". El objetivo es entender *por qué* el enfoque de EfficientNet es más equilibrado.

---

### **1. Definición de Nuestra Red Base ("BaseNet") y Coeficientes de Escalado**

Imaginemos que tenemos una red neuronal base, "BaseNet", con las siguientes características. También definiremos los coeficientes de escalado que EfficientNet encontró ($\alpha, \beta, \gamma$).

```python
# --- Parámetros de nuestra red base ---
base_depth = 18      # ej. 18 capas
base_width = 32      # ej. 32 filtros en una capa de referencia
base_resolution = 128  # ej. imágenes de entrada de 128x128

print("--- Dimensiones de BaseNet ---")
print(f"Profundidad: {base_depth}")
print(f"Ancho (Filtros): {base_width}")
print(f"Resolución: {base_resolution}x{base_resolution}")

# --- Coeficientes de Escalado de EfficientNet ---
# Estos son los valores aproximados del paper para escalar la red
alpha = 1.2  # Coeficiente para la profundidad
beta = 1.1   # Coeficiente para el ancho
gamma = 1.15 # Coeficiente para la resolución

# --- Coeficiente Compuesto (nuestro presupuesto de recursos) ---
# Elegimos un phi para escalar. Un phi=3 significa que queremos una red
# significativamente más grande que la base.
phi = 3.0
```

---
### **2. Estrategia de Escalado Simple**

Ahora, veamos qué pasa si usamos todos nuestros "recursos" ($\approx 2^\phi$) para escalar una sola dimensión.

#### **2.1. Escalado Simple: Solo Profundidad**
Hacemos la red mucho más profunda, pero mantenemos el ancho y la resolución originales.

```python
# Aumentamos la profundidad usando alpha y phi
scaled_depth_simple = base_depth * (alpha ** phi)

print("\n--- Escalado Simple (Solo Profundidad) ---")
print(f"Nueva Profundidad: {round(scaled_depth_simple)}")
print(f"Ancho (sin cambios): {base_width}")
print(f"Resolución (sin cambios): {base_resolution}")
print(">> Resultado: Una red muy profunda y 'delgada'. Podría tener problemas de gradientes y no ser capaz de capturar características complejas en cada capa.")
```

#### **2.2. Escalado Simple: Solo Ancho**
Hacemos la red mucho más ancha, pero con su profundidad y resolución originales.

```python
# Aumentamos el ancho usando beta y phi
scaled_width_simple = base_width * (beta ** phi)

print("\n--- Escalado Simple (Solo Ancho) ---")
print(f"Profundidad (sin cambios): {base_depth}")
print(f"Nuevo Ancho (Filtros): {round(scaled_width_simple)}")
print(f"Resolución (sin cambios): {base_resolution}")
print(">> Resultado: Una red muy ancha pero 'superficial'. Puede capturar características muy ricas en una capa, pero le falta la jerarquía para aprender conceptos abstractos.")
```

#### **2.3. Escalado Simple: Solo Resolución**
Usamos imágenes mucho más grandes, pero con la arquitectura original.

```python
# Aumentamos la resolución usando gamma y phi
scaled_resolution_simple = base_resolution * (gamma ** phi)

print("\n--- Escalado Simple (Solo Resolución) ---")
print(f"Profundidad (sin cambios): {base_depth}")
print(f"Ancho (sin cambios): {base_width}")
print(f"Nueva Resolución: {round(scaled_resolution_simple)}x{round(scaled_resolution_simple)}")
print(">> Resultado: El modelo recibe más detalles, pero su arquitectura 'pequeña' (poca profundidad y ancho) puede no tener la capacidad suficiente para procesarlos eficazmente.")
```

---
### **3. Estrategia de Escalado Compuesto (El Enfoque de EfficientNet)**

Ahora, aplicaremos el método de EfficientNet, distribuyendo los recursos de escalado ($\phi$) entre las tres dimensiones de manera equilibrada usando $\alpha, \beta, \gamma$.

```python
# Aplicamos las fórmulas de escalado compuesto
scaled_depth_compound = base_depth * (alpha ** phi)
scaled_width_compound = base_width * (beta ** phi)
scaled_resolution_compound = base_resolution * (gamma ** phi)

print("\n--- Escalado Compuesto (EfficientNet) ---")
print(f"Nueva Profundidad: {round(scaled_depth_compound)}")
print(f"Nuevo Ancho (Filtros): {round(scaled_width_compound)}")
print(f"Nueva Resolución: {round(scaled_resolution_compound)}x{round(scaled_resolution_compound)}")
print(">> Resultado: Una red que crece en todas sus dimensiones de manera armónica. Es más profunda, más ancha y procesa imágenes de mayor resolución, todo en proporción.")
```

---
### **4. Comparación y Conclusión**

Veamos los resultados en una tabla para apreciar la diferencia conceptual.

| Estrategia de Escalado | Profundidad | Ancho (Filtros) | Resolución | Observación |
| :----------------------- | :---------: | :-------------: | :--------: | :---------------------------------- |
| **BaseNet** | 18          | 32              | 128x128    | Modelo inicial.                     |
| **Simple (Solo Profundidad)** | **~31** | 32              | 128x128    | Desequilibrado: profundo y delgado. |
| **Simple (Solo Ancho)** | 18          | **~43** | 128x128    | Desequilibrado: ancho y superficial.|
| **Simple (Solo Resolución)** | 18          | 32              | **~195x195** | Desequilibrado: modelo pequeño para imágenes grandes. |
| **Compuesto (EfficientNet)** | **~31** | **~43** | **~195x195** | **Equilibrado:** Todas las dimensiones crecen juntas. |


**Conclusión Clave:** 💡

Este ejemplo numérico, sin necesidad de entrenar un modelo, ilustra la filosofía de EfficientNet. Mientras que el escalado simple crea arquitecturas "extremas" y desequilibradas, el **escalado compuesto produce un crecimiento armónico**. Esta armonía entre la capacidad de la red (profundidad y ancho) y la información que recibe (resolución) es la razón por la que EfficientNet logra una eficiencia y precisión tan notables.