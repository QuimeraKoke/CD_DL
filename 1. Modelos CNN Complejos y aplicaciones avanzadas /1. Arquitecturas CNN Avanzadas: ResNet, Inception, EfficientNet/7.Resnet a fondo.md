### **Arquitectura Detallada de un Bloque Residual y la Familia de Redes ResNet**

**Objetivo de la Clase:**

* Analizar en detalle la estructura interna de los bloques residuales m√°s comunes.
* Diferenciar entre el "bloque b√°sico" y el "bloque cuello de botella" (bottleneck) y entender sus respectivos usos.
* Comprender c√≥mo se apilan estos bloques para formar las arquitecturas ResNet completas, incluyendo el manejo del downsampling.
* Familiarizarse con la "familia ResNet" (ResNet-18, -34, -50, -101, -152) y qu√© significan estos n√∫meros.

---

**Contenido de la Clase:**

**(1) Recordando el Coraz√≥n de ResNet: El Bloque Residual** ‚ù§Ô∏è

Como vimos (¬°y experimentamos en el laboratorio!), la magia de ResNet reside en el **bloque residual**. La idea central es reformular el aprendizaje: en lugar de que un conjunto de capas aprenda directamente una transformaci√≥n $H(x)$, aprenden una funci√≥n residual $F(x)$ tal que la salida del bloque es $H(x) = F(x) + x$.

* **$x$**: La entrada al bloque (identidad).
* **$F(x)$**: La transformaci√≥n aprendida por las capas convolucionales dentro del bloque.
* **$+$**: La suma elemento a elemento realizada por la conexi√≥n directa (skip connection).

Esta estructura facilita que, si la transformaci√≥n √≥ptima es la identidad, $F(x)$ tienda a cero. Tambi√©n, como mencionamos, mejora significativamente el flujo de gradientes.

**(2) Anatom√≠a de un Bloque Residual Com√∫n** üî¨

Aunque hay variaciones, un bloque residual t√≠pico contiene:

* **Capas Convolucionales:** Generalmente dos o tres. Estas son las que componen $F(x)$.
* **Batch Normalization (BN):** Se aplica com√∫nmente despu√©s de cada capa convolucional y antes de la activaci√≥n. Ayuda a estabilizar el entrenamiento y regularizar.
* **Funciones de Activaci√≥n (ReLU):** Introducen no linealidad. En el dise√±o original de ResNet, la ReLU se aplica despu√©s de cada BN y tambi√©n *despu√©s* de la suma de la skip connection.
* **La Conexi√≥n Directa (Skip Connection):** El "atajo" que lleva $x$.
* **Suma Elemento a Elemento:** Donde $F(x)$ y $x$ se combinan.


![Diagrama Conceptual Detallado (ej. Bloque B√°sico)](.imgs/residual block.png)

**(3) Variaciones del Bloque Residual: B√°sico vs. Cuello de Botella (Bottleneck)**

Existen principalmente dos tipos de bloques residuales utilizados en las arquitecturas ResNet, dise√±ados para diferentes profundidades y eficiencias computacionales:

* **Bloque B√°sico (Basic Block):**
    * Utilizado en ResNets m√°s "superficiales" como ResNet-18 y ResNet-34.
    * Consta de **dos capas convolucionales de 3x3**.
    * Es computacionalmente m√°s directo.

* **Bloque Cuello de Botella (Bottleneck Block):**
    * Utilizado en ResNets m√°s profundas como ResNet-50, ResNet-101 y ResNet-152 para mayor eficiencia.
    * Consta de **tres capas convolucionales:**
        1.  **Convoluci√≥n 1x1:** Reduce la dimensionalidad (n√∫mero de filtros), por ejemplo, de 256 a 64 filtros. Esto es el "cuello de botella".
        2.  **Convoluci√≥n 3x3:** Realiza la convoluci√≥n principal sobre la representaci√≥n de menor dimensi√≥n (ej., con 64 filtros).
        3.  **Convoluci√≥n 1x1:** Restaura la dimensionalidad original (ej., de 64 a 256 filtros).
    * **¬øPor qu√© el cuello de botella?** La convoluci√≥n 3x3 es la m√°s costosa computacionalmente. Al reducir la dimensionalidad antes de la convoluci√≥n 3x3 y luego restaurarla, se reduce significativamente el n√∫mero total de operaciones y par√°metros, permitiendo construir redes m√°s profundas con un costo computacional manejable.

    Por ejemplo, para un bloque que maneja 256 filtros de salida, F\_bottleneck podr√≠a ser 64. El n√∫mero de filtros de salida D\_out suele ser 4 veces F\_bottleneck.

![Bloque B√°sico y Cuello de botella)](.imgs/resnet block.jpg)
    

**(4) Construyendo Redes ResNet Completas: Apilando Bloques** üèóÔ∏è

Las arquitecturas ResNet completas se construyen siguiendo un patr√≥n:

1.  **Capa Convolucional Inicial (Stem):** Una primera capa convolucional (ej., 7x7 con stride 2) seguida de Batch Normalization, ReLU y MaxPooling. Esta capa reduce r√°pidamente las dimensiones espaciales de la entrada.
2.  **Etapas de Bloques Residuales:** Se apilan varios bloques residuales en "etapas". T√≠picamente hay 4 etapas en las arquitecturas ResNet comunes.
    * Dentro de una etapa, todos los bloques suelen tener el mismo n√∫mero de filtros en su salida.
    * **Downsampling (Reducci√≥n de Dimensiones Espaciales):** La reducci√≥n de las dimensiones espaciales (equivalente a una capa de pooling) se realiza al inicio de cada etapa (excepto la primera, que sigue al stem que ya hizo pooling). Esto se logra usando un `stride` de 2 en la primera convoluci√≥n del primer bloque de la etapa.
    * **Ajuste de la Skip Connection durante Downsampling:** Cuando se realiza downsampling con `stride=2` en la ruta principal $F(x)$, la dimensi√≥n de $x$ en la skip connection ya no coincide con la de $F(x)$. Para solucionar esto, la skip connection tambi√©n debe realizar una transformaci√≥n para igualar las dimensiones. Esto se hace com√∫nmente con una convoluci√≥n 1x1 con `stride=2` en la skip connection (esto es el `use_projection=True` que vimos en el laboratorio).
3.  **Capa Final:**
    * **Global Average Pooling (GAP):** Despu√©s de la √∫ltima etapa de bloques residuales, se aplica GAP para reducir cada mapa de caracter√≠sticas a un solo n√∫mero.
    * **Capa Completamente Conectada (Dense):** Una capa densa final con activaci√≥n softmax para la clasificaci√≥n.

**(5) La Familia ResNet: ResNet-18, 34, 50, 101, 152...** üë®‚Äçüë©‚Äçüëß‚Äçüë¶

Los diferentes nombres en la familia ResNet (ResNet-18, ResNet-34, ResNet-50, etc.) se refieren al **n√∫mero total de capas convolucionales y la capa completamente conectada final** (no cuentan las capas de BN o de pooling como capas "numeradas" en este esquema).

* **ResNet-18, ResNet-34:** Usan **Bloques B√°sicos**.
* **ResNet-50, ResNet-101, ResNet-152:** Usan **Bloques Cuello de Botella**.

La profundidad se logra variando el n√∫mero de bloques en cada una de las 4 etapas. Por ejemplo (los n√∫meros son la cantidad de bloques por etapa):
* **ResNet-34:** Stem + [3, 4, 6, 3] bloques b√°sicos + FC
* **ResNet-50:** Stem + [3, 4, 6, 3] bloques cuello de botella + FC

Comprender estos patrones de dise√±o nos permite no solo usar arquitecturas pre-entrenadas, sino tambi√©n tener una idea de c√≥mo se pueden construir o modificar arquitecturas profundas de manera efectiva.

**Preguntas para la Reflexi√≥n:**

* ¬øPor qu√© es crucial que la skip connection se ajuste (ej. con una convoluci√≥n 1x1) cuando hay un cambio de dimensi√≥n en la ruta principal del bloque?
* Si tienes un presupuesto computacional limitado pero necesitas una red profunda, ¬øqu√© tipo de bloque residual (b√°sico o cuello de botella) preferir√≠as y por qu√©?

---