## Laboratorio Práctico: El Poder de los Bloques Residuales 💡

**Objetivo del Laboratorio:**

* Implementar y entrenar una Red Neuronal Convolucional (CNN) profunda "plana" (sin conexiones residuales).
* Implementar y entrenar una CNN con una profundidad comparable utilizando bloques residuales.
* Comparar el proceso de entrenamiento y el rendimiento de ambas arquitecturas en un conjunto de datos estándar (CIFAR-10).
* Observar experimentalmente cómo los bloques residuales pueden ayudar a entrenar redes más profundas y mitigar problemas como el de degradación.

---

**Entorno:** Python, TensorFlow, Keras, Matplotlib

```python
# Importar las librerías necesarias
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import numpy as np

# Configuración para asegurar reproducibilidad (opcional, pero buena práctica)
# tf.random.set_seed(42)
# np.random.seed(42)
```

---

### Parte 0: Carga y Preparación del Dataset (CIFAR-10)

CIFAR-10 es un dataset comúnmente usado con 10 clases de imágenes de 32x32 píxeles.

```python
# Cargar el dataset CIFAR-10
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()

# Normalizar los valores de los píxeles al rango [0, 1]
x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0

# Convertir las etiquetas a formato one-hot encoding
num_classes = 10
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

print(f"Forma de x_train: {x_train.shape}")
print(f"Forma de y_train: {y_train.shape}")
```

---

### Parte 1: Construyendo y Entrenando una CNN "Plana" Profunda (PlainNet)

Vamos a construir una CNN secuencial con varias capas. Intentaremos hacerla lo suficientemente profunda como para que pueda empezar a mostrar dificultades en el entrenamiento sin conexiones residuales.

**Arquitectura de PlainNet:**

Usaremos bloques `Conv2D -> BatchNormalization -> ReLU`.
* Bloque 1: 3 capas convolucionales con 32 filtros, seguidas de MaxPooling.
* Bloque 2: 3 capas convolucionales con 64 filtros, seguidas de MaxPooling.
* Bloque 3: 3 capas convolucionales con 128 filtros.
* GlobalAveragePooling y una capa Densa para la clasificación.

```python
def build_plain_cnn(input_shape, num_classes):
    inputs = keras.Input(shape=input_shape)

    # Bloque 1
    x = layers.Conv2D(32, (3, 3), padding="same", activation="relu")(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(32, (3, 3), padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(32, (3, 3), padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D((2, 2))(x)

    # Bloque 2
    x = layers.Conv2D(64, (3, 3), padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(64, (3, 3), padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(64, (3, 3), padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D((2, 2))(x)

    # Bloque 3
    x = layers.Conv2D(128, (3, 3), padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(128, (3, 3), padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(128, (3, 3), padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)

    # Clasificador
    x = layers.GlobalAveragePooling2D()(x)
    outputs = layers.Dense(num_classes, activation="softmax")(x)

    model = keras.Model(inputs=inputs, outputs=outputs, name="plain_cnn")
    return model

# Crear el modelo PlainNet
plain_model = build_plain_cnn(x_train.shape[1:], num_classes)
plain_model.summary()

# Compilar el modelo
plain_model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    loss="categorical_crossentropy",
    metrics=["accuracy"],
)

# Entrenar el modelo
print("\nEntrenando PlainNet...")
batch_size = 64
epochs = 20 # Puedes aumentar las épocas si tienes tiempo/GPU

history_plain = plain_model.fit(
    x_train, y_train,
    batch_size=batch_size,
    epochs=epochs,
    validation_data=(x_test, y_test)
)
```

---

### Parte 2: Construyendo y Entrenando una CNN con Bloques Residuales (SimpleResNet)

Ahora, definiremos un bloque residual y construiremos una red de profundidad comparable usando estos bloques.

**Función para el Bloque Residual:**

```python
def residual_block(x, filters, kernel_size=3, stride=1, use_projection=False):
    # Ruta principal (conv -> bn -> relu -> conv -> bn)
    y = layers.Conv2D(filters, kernel_size=kernel_size, strides=stride, padding="same")(x)
    y = layers.BatchNormalization()(y)
    y = layers.ReLU()(y)
    y = layers.Conv2D(filters, kernel_size=kernel_size, strides=1, padding="same")(y) # La segunda conv siempre tiene stride 1
    y = layers.BatchNormalization()(y)

    # Ruta de la conexión directa (shortcut)
    shortcut = x
    if use_projection: # Se usa si hay cambio de dimensión (stride > 1) o cambio en el número de filtros
        shortcut = layers.Conv2D(filters, kernel_size=1, strides=stride, padding="same")(x)
        shortcut = layers.BatchNormalization()(shortcut)

    # Sumar la salida de la ruta principal y la conexión directa
    out = layers.add([shortcut, y])
    out = layers.ReLU()(out) # Aplicar ReLU después de la suma
    return out
```

**Arquitectura de SimpleResNet:**

* Capa Convolucional Inicial (Stem).
* Serie de Bloques Residuales:
    * 2 bloques con 32 filtros.
    * 2 bloques con 64 filtros (el primero con `stride=2` y `use_projection=True` para downsampling).
    * 2 bloques con 128 filtros (el primero con `stride=2` y `use_projection=True` para downsampling).
* GlobalAveragePooling y una capa Densa para la clasificación.

```python
def build_resnet(input_shape, num_classes):
    inputs = keras.Input(shape=input_shape)

    # Capa inicial (Stem)
    x = layers.Conv2D(32, (3, 3), strides=1, padding="same")(inputs) # Stride 1 para mantener tamaño
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)
    # x = layers.MaxPooling2D((2,2), padding="same")(x) # Opcional: un maxpooling inicial

    # Bloques Residuales
    # Etapa 1
    x = residual_block(x, filters=32)
    x = residual_block(x, filters=32)

    # Etapa 2 (con downsampling)
    x = residual_block(x, filters=64, stride=2, use_projection=True)
    x = residual_block(x, filters=64)

    # Etapa 3 (con downsampling)
    x = residual_block(x, filters=128, stride=2, use_projection=True)
    x = residual_block(x, filters=128)

    # Clasificador
    x = layers.GlobalAveragePooling2D()(x)
    outputs = layers.Dense(num_classes, activation="softmax")(x)

    model = keras.Model(inputs=inputs, outputs=outputs, name="simple_resnet")
    return model

# Crear el modelo SimpleResNet
resnet_model = build_resnet(x_train.shape[1:], num_classes)
resnet_model.summary()

# Compilar el modelo
resnet_model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    loss="categorical_crossentropy",
    metrics=["accuracy"],
)

# Entrenar el modelo
print("\nEntrenando SimpleResNet...")
# batch_size y epochs definidos anteriormente

history_resnet = resnet_model.fit(
    x_train, y_train,
    batch_size=batch_size,
    epochs=epochs,
    validation_data=(x_test, y_test)
)
```

---

### Parte 3: Comparación y Análisis de Resultados

Ahora vamos a graficar las curvas de aprendizaje para comparar ambos modelos.

```python
plt.figure(figsize=(14, 5))

# Gráfica de Pérdida (Loss)
plt.subplot(1, 2, 1)
plt.plot(history_plain.history['loss'], label='PlainNet Training Loss')
plt.plot(history_plain.history['val_loss'], label='PlainNet Validation Loss')
plt.plot(history_resnet.history['loss'], label='ResNet Training Loss', linestyle='--')
plt.plot(history_resnet.history['val_loss'], label='ResNet Validation Loss', linestyle='--')
plt.title('Comparación de Pérdida (Loss)')
plt.xlabel('Épocas')
plt.ylabel('Pérdida')
plt.legend()
plt.grid(True)

# Gráfica de Precisión (Accuracy)
plt.subplot(1, 2, 2)
plt.plot(history_plain.history['accuracy'], label='PlainNet Training Accuracy')
plt.plot(history_plain.history['val_accuracy'], label='PlainNet Validation Accuracy')
plt.plot(history_resnet.history['accuracy'], label='ResNet Training Accuracy', linestyle='--')
plt.plot(history_resnet.history['val_accuracy'], label='ResNet Validation Accuracy', linestyle='--')
plt.title('Comparación de Precisión (Accuracy)')
plt.xlabel('Épocas')
plt.ylabel('Precisión')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# Comparar la precisión final en el conjunto de prueba
loss_plain, acc_plain = plain_model.evaluate(x_test, y_test, verbose=0)
loss_resnet, acc_resnet = resnet_model.evaluate(x_test, y_test, verbose=0)

print(f"\nPlainNet - Precisión en Test: {acc_plain*100:.2f}%")
print(f"SimpleResNet - Precisión en Test: {acc_resnet*100:.2f}%")
```

---

**Preguntas para el Análisis y Discusión:**

1.  **Observa las curvas de entrenamiento (pérdida y precisión):**
    * ¿Cuál de los modelos parece converger más rápido o a un mejor valor de pérdida/precisión en el conjunto de entrenamiento?
    * ¿Cómo se comparan las curvas de validación? ¿Alguno de los modelos muestra signos de sobreajuste más rápido?
    * ¿Observas alguna dificultad particular en el entrenamiento de la `PlainNet` (ej. estancamiento temprano, fluctuaciones)?
2.  **Comparación de la Precisión Final:**
    * ¿Cuál modelo obtuvo una mejor precisión en el conjunto de prueba?
    * ¿La diferencia es significativa?
3.  **Profundidad y Degradación:**
    * Aunque la `PlainNet` no es extremadamente profunda, ¿crees que los resultados insinúan el "problema de degradación" que se discutió? ¿Por qué?
    * ¿Cómo crees que los bloques residuales ayudaron al `SimpleResNet` a entrenar mejor (si es que lo hizo)?
4.  **Experimentación (Opcional):**
    * ¿Qué pasaría si intentaras hacer la `PlainNet` aún más profunda (añadiendo más bloques convolucionales)? ¿Y si hicieras lo mismo con `SimpleResNet`?
    * ¿Cómo afectaría cambiar el optimizador o la tasa de aprendizaje?

---