## Laboratorio Pr치ctico: El Poder de los Bloques Residuales 游눠

**Objetivo del Laboratorio:**

* Implementar y entrenar una Red Neuronal Convolucional (CNN) profunda "plana" (sin conexiones residuales).
* Implementar y entrenar una CNN con una profundidad comparable utilizando bloques residuales.
* Comparar el proceso de entrenamiento y el rendimiento de ambas arquitecturas en un conjunto de datos est치ndar (CIFAR-10).
* Observar experimentalmente c칩mo los bloques residuales pueden ayudar a entrenar redes m치s profundas y mitigar problemas como el de degradaci칩n.

---

**Entorno:** Python, TensorFlow, Keras, Matplotlib

```python
# Importar las librer칤as necesarias
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import numpy as np

# Configuraci칩n para asegurar reproducibilidad (opcional, pero buena pr치ctica)
# tf.random.set_seed(42)
# np.random.seed(42)
```

---

### Parte 0: Carga y Preparaci칩n del Dataset (CIFAR-10)

CIFAR-10 es un dataset com칰nmente usado con 10 clases de im치genes de 32x32 p칤xeles.

```python
# Cargar el dataset CIFAR-10
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()

# Normalizar los valores de los p칤xeles al rango [0, 1]
x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0

# Convertir las etiquetas a formato one-hot encoding
num_classes = 10
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

print(f"Forma de x_train: {x_train.shape}")
print(f"Forma de y_train: {y_train.shape}")
```

---

### Parte 1: Construyendo y Entrenando una CNN "Plana" Profunda (PlainNet)

Vamos a construir una CNN secuencial con varias capas. Intentaremos hacerla lo suficientemente profunda como para que pueda empezar a mostrar dificultades en el entrenamiento sin conexiones residuales.

**Arquitectura de PlainNet:**

Usaremos bloques `Conv2D -> BatchNormalization -> ReLU`.
* Bloque 1: 3 capas convolucionales con 32 filtros, seguidas de MaxPooling.
* Bloque 2: 3 capas convolucionales con 64 filtros, seguidas de MaxPooling.
* Bloque 3: 3 capas convolucionales con 128 filtros.
* GlobalAveragePooling y una capa Densa para la clasificaci칩n.

```python
def build_plain_cnn(input_shape, num_classes):
    inputs = keras.Input(shape=input_shape)

    # Bloque 1
    x = layers.Conv2D(32, (3, 3), padding="same", activation="relu")(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(32, (3, 3), padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(32, (3, 3), padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D((2, 2))(x)

    # Bloque 2
    x = layers.Conv2D(64, (3, 3), padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(64, (3, 3), padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(64, (3, 3), padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D((2, 2))(x)

    # Bloque 3
    x = layers.Conv2D(128, (3, 3), padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(128, (3, 3), padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(128, (3, 3), padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)

    # Clasificador
    x = layers.GlobalAveragePooling2D()(x)
    outputs = layers.Dense(num_classes, activation="softmax")(x)

    model = keras.Model(inputs=inputs, outputs=outputs, name="plain_cnn")
    return model

# Crear el modelo PlainNet
plain_model = build_plain_cnn(x_train.shape[1:], num_classes)
plain_model.summary()

# Compilar el modelo
plain_model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    loss="categorical_crossentropy",
    metrics=["accuracy"],
)

# Entrenar el modelo
print("\nEntrenando PlainNet...")
batch_size = 64
epochs = 20 # Puedes aumentar las 칠pocas si tienes tiempo/GPU

history_plain = plain_model.fit(
    x_train, y_train,
    batch_size=batch_size,
    epochs=epochs,
    validation_data=(x_test, y_test)
)
```

---

### Parte 2: Construyendo y Entrenando una CNN con Bloques Residuales (SimpleResNet)

Ahora, definiremos un bloque residual y construiremos una red de profundidad comparable usando estos bloques.

**Funci칩n para el Bloque Residual:**

```python
def residual_block(x, filters, kernel_size=3, stride=1, use_projection=False):
    # Ruta principal (conv -> bn -> relu -> conv -> bn)
    y = layers.Conv2D(filters, kernel_size=kernel_size, strides=stride, padding="same")(x)
    y = layers.BatchNormalization()(y)
    y = layers.ReLU()(y)
    y = layers.Conv2D(filters, kernel_size=kernel_size, strides=1, padding="same")(y) # La segunda conv siempre tiene stride 1
    y = layers.BatchNormalization()(y)

    # Ruta de la conexi칩n directa (shortcut)
    shortcut = x
    if use_projection: # Se usa si hay cambio de dimensi칩n (stride > 1) o cambio en el n칰mero de filtros
        shortcut = layers.Conv2D(filters, kernel_size=1, strides=stride, padding="same")(x)
        shortcut = layers.BatchNormalization()(shortcut)

    # Sumar la salida de la ruta principal y la conexi칩n directa
    out = layers.add([shortcut, y])
    out = layers.ReLU()(out) # Aplicar ReLU despu칠s de la suma
    return out
```

**Arquitectura de SimpleResNet:**

* Capa Convolucional Inicial (Stem).
* Serie de Bloques Residuales:
    * 2 bloques con 32 filtros.
    * 2 bloques con 64 filtros (el primero con `stride=2` y `use_projection=True` para downsampling).
    * 2 bloques con 128 filtros (el primero con `stride=2` y `use_projection=True` para downsampling).
* GlobalAveragePooling y una capa Densa para la clasificaci칩n.

```python
def build_resnet(input_shape, num_classes):
    inputs = keras.Input(shape=input_shape)

    # Capa inicial (Stem)
    x = layers.Conv2D(32, (3, 3), strides=1, padding="same")(inputs) # Stride 1 para mantener tama침o
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)
    # x = layers.MaxPooling2D((2,2), padding="same")(x) # Opcional: un maxpooling inicial

    # Bloques Residuales
    # Etapa 1
    x = residual_block(x, filters=32)
    x = residual_block(x, filters=32)

    # Etapa 2 (con downsampling)
    x = residual_block(x, filters=64, stride=2, use_projection=True)
    x = residual_block(x, filters=64)

    # Etapa 3 (con downsampling)
    x = residual_block(x, filters=128, stride=2, use_projection=True)
    x = residual_block(x, filters=128)

    # Clasificador
    x = layers.GlobalAveragePooling2D()(x)
    outputs = layers.Dense(num_classes, activation="softmax")(x)

    model = keras.Model(inputs=inputs, outputs=outputs, name="simple_resnet")
    return model

# Crear el modelo SimpleResNet
resnet_model = build_resnet(x_train.shape[1:], num_classes)
resnet_model.summary()

# Compilar el modelo
resnet_model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    loss="categorical_crossentropy",
    metrics=["accuracy"],
)

# Entrenar el modelo
print("\nEntrenando SimpleResNet...")
# batch_size y epochs definidos anteriormente

history_resnet = resnet_model.fit(
    x_train, y_train,
    batch_size=batch_size,
    epochs=epochs,
    validation_data=(x_test, y_test)
)
```

---

### Parte 3: Comparaci칩n y An치lisis de Resultados

Ahora vamos a graficar las curvas de aprendizaje para comparar ambos modelos.

```python
plt.figure(figsize=(14, 5))

# Gr치fica de P칠rdida (Loss)
plt.subplot(1, 2, 1)
plt.plot(history_plain.history['loss'], label='PlainNet Training Loss')
plt.plot(history_plain.history['val_loss'], label='PlainNet Validation Loss')
plt.plot(history_resnet.history['loss'], label='ResNet Training Loss', linestyle='--')
plt.plot(history_resnet.history['val_loss'], label='ResNet Validation Loss', linestyle='--')
plt.title('Comparaci칩n de P칠rdida (Loss)')
plt.xlabel('칄pocas')
plt.ylabel('P칠rdida')
plt.legend()
plt.grid(True)

# Gr치fica de Precisi칩n (Accuracy)
plt.subplot(1, 2, 2)
plt.plot(history_plain.history['accuracy'], label='PlainNet Training Accuracy')
plt.plot(history_plain.history['val_accuracy'], label='PlainNet Validation Accuracy')
plt.plot(history_resnet.history['accuracy'], label='ResNet Training Accuracy', linestyle='--')
plt.plot(history_resnet.history['val_accuracy'], label='ResNet Validation Accuracy', linestyle='--')
plt.title('Comparaci칩n de Precisi칩n (Accuracy)')
plt.xlabel('칄pocas')
plt.ylabel('Precisi칩n')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# Comparar la precisi칩n final en el conjunto de prueba
loss_plain, acc_plain = plain_model.evaluate(x_test, y_test, verbose=0)
loss_resnet, acc_resnet = resnet_model.evaluate(x_test, y_test, verbose=0)

print(f"\nPlainNet - Precisi칩n en Test: {acc_plain*100:.2f}%")
print(f"SimpleResNet - Precisi칩n en Test: {acc_resnet*100:.2f}%")
```

---

**Preguntas para el An치lisis y Discusi칩n:**

1.  **Observa las curvas de entrenamiento (p칠rdida y precisi칩n):**
    * 쮺u치l de los modelos parece converger m치s r치pido o a un mejor valor de p칠rdida/precisi칩n en el conjunto de entrenamiento?
    * 쮺칩mo se comparan las curvas de validaci칩n? 쮸lguno de los modelos muestra signos de sobreajuste m치s r치pido?
    * 쯆bservas alguna dificultad particular en el entrenamiento de la `PlainNet` (ej. estancamiento temprano, fluctuaciones)?
2.  **Comparaci칩n de la Precisi칩n Final:**
    * 쮺u치l modelo obtuvo una mejor precisi칩n en el conjunto de prueba?
    * 쯃a diferencia es significativa?
3.  **Profundidad y Degradaci칩n:**
    * Aunque la `PlainNet` no es extremadamente profunda, 쯖rees que los resultados insin칰an el "problema de degradaci칩n" que se discuti칩? 쯇or qu칠?
    * 쮺칩mo crees que los bloques residuales ayudaron al `SimpleResNet` a entrenar mejor (si es que lo hizo)?
4.  **Experimentaci칩n (Opcional):**
    * 쯈u칠 pasar칤a si intentaras hacer la `PlainNet` a칰n m치s profunda (a침adiendo m치s bloques convolucionales)? 쯏 si hicieras lo mismo con `SimpleResNet`?
    * 쮺칩mo afectar칤a cambiar el optimizador o la tasa de aprendizaje?

---