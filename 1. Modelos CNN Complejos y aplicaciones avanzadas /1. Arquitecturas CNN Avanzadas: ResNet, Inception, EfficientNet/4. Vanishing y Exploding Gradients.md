### **Vanishing & Exploding Gradients: Identificaci√≥n y Consecuencias**

**Objetivo de la Clase:**

* Definir y comprender el problema de la "desaparici√≥n de gradientes" (vanishing gradients).
* Definir y comprender el problema de la "explosi√≥n de gradientes" (exploding gradients).
* Identificar las causas comunes y las consecuencias de ambos problemas en el entrenamiento de redes neuronales profundas.
* Aprender a reconocer los s√≠ntomas de estos problemas durante el entrenamiento.

---

**Contenido de la Clase:**

**(1) Recordatorio: Gradientes y Backpropagation** üåä

Como sabemos, las redes neuronales aprenden mediante un proceso llamado **backpropagation** (retropropagaci√≥n del error). Durante este proceso:
1. Se calcula el error (o p√©rdida) en la salida de la red.
2. Este error se propaga hacia atr√°s, capa por capa.
3. En cada capa, se calculan los **gradientes** de la funci√≥n de p√©rdida con respecto a los pesos de esa capa.
4. Estos gradientes nos dicen c√≥mo ajustar los pesos para reducir el error. Un gradiente grande significa que un peque√±o cambio en el peso tendr√° un gran impacto en el error; un gradiente peque√±o significa un impacto menor.

En redes profundas, estos gradientes deben propagarse a trav√©s de muchas capas. Aqu√≠ es donde pueden surgir problemas.

**(2) El Problema de la Vanishing Gradients** üëªüìâ

**¬øQu√© es?**
El problema de la desaparici√≥n de gradientes ocurre cuando los gradientes de la funci√≥n de p√©rdida con respecto a los pesos de las primeras capas de la red se vuelven **extremadamente peque√±os** (tienden a cero) a medida que se propagan hacia atr√°s desde la capa de salida.

**Causas Comunes:**
* **Funciones de Activaci√≥n Saturantes:** Funciones como la sigmoide o la tangente hiperb√≥lica (tanh) tienen derivadas que son muy peque√±as cuando sus entradas son muy grandes o muy peque√±as (es decir, cuando est√°n en sus regiones de saturaci√≥n). Durante el backpropagation, estos peque√±os valores de las derivadas se multiplican entre s√≠ a trav√©s de las capas. Si tienes muchas capas con estas activaciones, el producto de muchos n√∫meros peque√±os se vuelve ¬°extremadamente peque√±o!
* **Multiplicaci√≥n en Cadena:** En general, el c√°lculo del gradiente en las primeras capas implica el producto de muchas derivadas parciales (una por cada capa posterior). Si estos t√©rminos son consistentemente menores que 1, su producto se desvanece exponencialmente con el n√∫mero de capas.

**Consecuencias:**
* **Aprendizaje Lento o Estancado:** Si los gradientes son diminutos, las actualizaciones de los pesos en las primeras capas ser√°n min√∫sculas. Estas capas, que a menudo son responsables de aprender caracter√≠sticas fundamentales, apenas aprender√°n o lo har√°n muy lentamente.
* **Las Primeras Capas No Aprenden:** La red se vuelve incapaz de aprender relaciones complejas que dependen de las caracter√≠sticas detectadas por las capas iniciales.
* **Convergencia Prematura a Soluciones Sub√≥ptimas:** El modelo puede parecer que converge, pero a una soluci√≥n de baja calidad porque gran parte de la red no se entren√≥ adecuadamente.

**Identificaci√≥n (S√≠ntomas):**
* El entrenamiento es excesivamente lento a pesar de una tasa de aprendizaje adecuada.
* Los pesos y sesgos de las primeras capas cambian muy poco o nada entre √©pocas.
* La p√©rdida de entrenamiento se estanca r√°pidamente en un valor alto.
* El rendimiento en el conjunto de validaci√≥n es pobre y no mejora.

**(3) El Problema de la Exploding Gradients** üí£üìà

**¬øQu√© es?**
El problema de la explosi√≥n de gradientes es lo opuesto: los gradientes crecen **excesivamente grandes** (tienden a infinito) a medida que se propagan hacia atr√°s.

**Causas Comunes:**
* **Inicializaci√≥n de Pesos Inadecuada:** Si los pesos iniciales de la red son demasiado grandes.
* **Multiplicaci√≥n en Cadena:** Similar al caso de la desaparici√≥n, si los t√©rminos en el producto de las derivadas parciales son consistentemente mayores que 1, su producto puede crecer exponencialmente.
* **Una Tasa de Aprendizaje Demasiado Alta:** Puede exacerbar el problema.

**Consecuencias:**
* **Inestabilidad Num√©rica:** Los gradientes enormes pueden llevar a actualizaciones de pesos tan grandes que los valores de los pesos se vuelven extremadamente grandes.
* **Oscilaciones Violentas en la P√©rdida:** La funci√≥n de p√©rdida puede oscilar salvajemente o aumentar en lugar de disminuir.
* **Divergencia del Entrenamiento:** El modelo no logra converger y el entrenamiento falla por completo.

**Identificaci√≥n (S√≠ntomas):**
* La funci√≥n de p√©rdida se convierte r√°pidamente en infinito.
* Los valores de los pesos crecen a magnitudes muy grandes.
* La curva de p√©rdida muestra picos y valles err√°ticos y muy pronunciados.

![Gr√°fico que representan el vanishing y el exploding](/img/vanishing-and-exploding.webp)

**(4) Un Equilibrio Delicado**

Entrenar redes profundas es como caminar por una cuerda floja con los gradientes. Necesitamos que sean lo suficientemente grandes para que las capas aprendan, pero no tan grandes como para que el entrenamiento se vuelva inestable.

**(5) Pistas Hacia las Soluciones (Que Veremos M√°s Adelante)**

Afortunadamente, la comunidad de Deep Learning ha desarrollado varias estrategias para combatir estos problemas:

* **Inicializaci√≥n de Pesos Cuidadosa:** (Ej. Xavier/Glorot, He)
* **Funciones de Activaci√≥n No Saturantes:** (Ej. ReLU y sus variantes)
* **Batch Normalization:** Ayuda a mantener las activaciones en rangos m√°s estables.
* **Gradient Clipping:** Una t√©cnica para "recortar" los gradientes si exceden un cierto umbral (especialmente √∫til para la explosi√≥n de gradientes).
* **Arquitecturas Espec√≠ficas:** Como las **ResNet** con sus skip connections, que proporcionan rutas alternativas para que los gradientes fluyan, ayudando a mitigar la desaparici√≥n de gradientes.

Comprender la desaparici√≥n y explosi√≥n de gradientes es fundamental para diagnosticar problemas de entrenamiento y para apreciar por qu√© ciertas arquitecturas y t√©cnicas son tan efectivas en el Deep Learning moderno.

**Preguntas para la Reflexi√≥n:**

* ¬øPor qu√© una funci√≥n de activaci√≥n como ReLU es menos propensa a causar el problema de desaparici√≥n de gradientes en comparaci√≥n con la sigmoide, para entradas positivas?
* Si ves que tu p√©rdida se vuelve `NaN` durante el entrenamiento, ¬øcu√°l de los dos problemas (desaparici√≥n o explosi√≥n) sospechar√≠as primero y por qu√©?

---
