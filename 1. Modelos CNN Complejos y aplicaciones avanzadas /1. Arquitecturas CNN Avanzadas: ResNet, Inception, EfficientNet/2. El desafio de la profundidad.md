### **El Desaf칤o de la Profundidad: 쯇or Qu칠 M치s Capas No Siempre Es Mejor?**

**Objetivo de la Clase:**

* Cuestionar la suposici칩n intuitiva de que "m치s capas siempre mejoran el rendimiento".
* Introducir el fen칩meno observado de que redes "planas" (plain networks) muy profundas pueden tener un rendimiento inferior a redes m치s superficiales.
* Distinguir este problema del sobreajuste (overfitting) com칰n.
* Preparar el terreno para entender los problemas espec칤ficos como la degradaci칩n y la desaparici칩n/explosi칩n de gradientes.

---

**Contenido de la Clase:**

**(1) La Intuici칩n Inicial: M치s Capas, M치s Poder** 游뱂

Como vimos en la clase anterior, la idea de apilar m치s capas en una CNN es atractiva. Intuitivamente, pensamos:

* M치s capas significan una **jerarqu칤a de caracter칤sticas m치s rica y abstracta**.
* M치s capas implican una **mayor capacidad del modelo** para aprender funciones complejas.

Siguiendo esta l칩gica, uno esperar칤a que una red con, digamos, 50 capas siempre supere a una con 20 capas, asumiendo que ambas pueden ser entrenadas.

**(2) La Sorprendente Realidad: Cuando la Profundidad Perjudica** 游늴

Sin embargo, los investigadores que comenzaron a experimentar con redes significativamente m치s profundas (mucho antes de arquitecturas como ResNet) observaron un fen칩meno desconcertante:

Al tomar una red "plana" (una red secuencial simple donde las capas se apilan una tras otra sin "atajos" o conexiones especiales) y simplemente a침adirle m치s capas, el rendimiento no solo dejaba de mejorar, sino que **춰pod칤a empezar a empeorar!**

* Una red de 56 capas pod칤a tener un **error de entrenamiento y de prueba m치s alto** que una red similar de solo 20 capas.

Esto es contraintuitivo. Si una red m치s profunda tiene mayor capacidad, 쯣or qu칠 no puede, como m칤nimo, aprender lo mismo que una red m치s superficial y luego a침adir m치s transformaciones 칰tiles? Te칩ricamente, una red m치s profunda deber칤a ser capaz de aprender la funci칩n identidad para las capas adicionales y replicar el rendimiento de la red m치s superficial. Pero en la pr치ctica, esto no suced칤a f치cilmente.

**(3) No Es (Solo) Sobreajuste** 丘멆잺

Es importante distinguir este problema del **sobreajuste (overfitting)**.

* **Sobreajuste:** Ocurre cuando un modelo aprende muy bien los datos de entrenamiento (error de entrenamiento bajo) pero no generaliza bien a datos nuevos (error de prueba alto).
* **El Desaf칤o de la Profundidad (antes de ResNet):** Se manifestaba con un **error de entrenamiento m치s alto** para la red m치s profunda. Esto indica que la red m치s profunda ni siquiera estaba logrando aprender bien los datos con los que fue entrenada, en comparaci칩n con su contraparte m치s superficial.

Si el problema fuera solo sobreajuste, el error de entrenamiento de la red profunda ser칤a bajo. El hecho de que el error de *entrenamiento* aumentara indicaba un problema m치s fundamental con la optimizaci칩n o la capacidad de estas redes profundas "planas" para aprender eficazmente.

**(4) Un Misterio a Resolver** 游댌

Este comportamiento inesperado de las redes profundas "planas" fue un obst치culo significativo. Indicaba que simplemente hacer las redes m치s grandes y profundas no era la respuesta directa para mejorar el rendimiento.

Este desaf칤o impuls칩 la investigaci칩n hacia:

* Comprender *por qu칠* suced칤a esto (lo que nos llevar치 a las clases sobre el problema de degradaci칩n y los gradientes).
* Desarrollar nuevas arquitecturas y t칠cnicas que permitieran entrenar redes mucho m치s profundas de manera efectiva.

**Preguntas para Reflexionar:**

* 쯇or qu칠 es tan sorprendente que una red m치s profunda pueda tener un error de *entrenamiento* m치s alto que una m치s superficial?
* Si no es sobreajuste, 쯤u칠 otras dificultades podr칤an surgir al intentar entrenar una red muy, muy profunda?

---