## Laboratorio Pr치ctico: Transfer Learning con Extracci칩n de Caracter칤sticas

### **Objetivo del Laboratorio**
* Implementar la t칠cnica de **extracci칩n de caracter칤sticas** usando un modelo pre-entrenado (EfficientNetV2).
* Cargar un modelo convolucional (EfficientNetV2), **congelar** sus pesos y a침adirle un nuevo clasificador.
* Entrenar el nuevo clasificador en un dataset diferente (CIFAR-10) y lograr una alta precisi칩n con muy poco tiempo de entrenamiento.
* Entender en la pr치ctica los beneficios de no empezar desde cero.

### **Entorno**
Este laboratorio utiliza **Python**, **TensorFlow** y **Keras**.

---
### **Parte 0: Configuraci칩n y Preparaci칩n del Dataset**
Comenzamos con la configuraci칩n habitual, importando librer칤as y preparando el dataset CIFAR-10.

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt

# Cargar el dataset CIFAR-10
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()

# Normalizar los valores de los p칤xeles
# EfficientNetV2 no requiere normalizaci칩n a [0, 1],
# ya que su capa de preprocesamiento interna se encarga de ello.
# Solo nos aseguramos de que el tipo de dato sea correcto.
x_train = x_train.astype("float32")
x_test = x_test.astype("float32")

# Convertir las etiquetas a formato one-hot encoding
num_classes = 10
y_train_cat = keras.utils.to_categorical(y_train, num_classes)
y_test_cat = keras.utils.to_categorical(y_test, num_classes)

print(f"Forma de x_train: {x_train.shape}")
print(f"Forma de y_train (categ칩rica): {y_train_cat.shape}")
```

---
### **Parte 1: Cargar el Modelo Pre-entrenado (La Base Convolucional)**
Cargaremos `EfficientNetV2B0`, un modelo moderno y muy eficiente. La clave aqu칤 es usar dos argumentos importantes:
* `weights='imagenet'`: Para cargar los pesos ya entrenados.
* `include_top=False`: Para **descartar** la capa de clasificaci칩n original de ImageNet (que ten칤a 1000 clases).
* `input_shape`: Para adaptar el modelo a nuestras im치genes de 32x32x3.

```python
# Definir la forma de entrada
INPUT_SHAPE = (32, 32, 3)

# Cargar la base del modelo EfficientNetV2B0
base_model = keras.applications.EfficientNetV2B0(
    include_top=False, # No incluir el clasificador original
    weights='imagenet',
    input_shape=INPUT_SHAPE
)

print("Modelo base cargado exitosamente.")
```
---
### **Parte 2: Congelar la Base Convolucional 游븱**
Este es el paso m치s importante en la extracci칩n de caracter칤sticas. Le decimos a Keras que **no queremos re-entrenar** la base convolucional; solo la usaremos para extraer caracter칤sticas. Sus pesos deben permanecer "congelados".

```python
# Congelar la base del modelo
base_model.trainable = False

print("La base del modelo ha sido congelada.")

# Verifiquemos el estado
# base_model.summary() # Descomentar para ver el gran n칰mero de par치metros no entrenables
```
---
### **Parte 3: Construir el Nuevo Modelo con Nuestra Propia "Cabeza"**
Ahora, construiremos un nuevo modelo `Sequential` que usar치 la base congelada como su primera capa y le a침adiremos nuestro propio clasificador (la "cabeza") al final.

```python
# Construir el modelo final
model = keras.Sequential([
    # 1. La base convolucional congelada
    base_model,
    
    # 2. Capa de pooling para aplanar los mapas de caracter칤sticas
    layers.GlobalAveragePooling2D(),
    
    # 3. Capa de Dropout para regularizaci칩n
    layers.Dropout(0.3),
    
    # 4. Nuestra capa de clasificaci칩n final
    layers.Dense(num_classes, activation="softmax")
], name="transfer_learning_model")


# Imprimir el resumen del nuevo modelo
# Observa la diferencia entre "Total params" y "Trainable params"
model.summary()
```
**Observaci칩n Clave:** F칤jate en el resumen del modelo. Ver치s millones de par치metros, 춰pero la mayor칤a son "Non-trainable"! Solo entrenaremos los par치metros de la nueva capa `Dense`, que son muy pocos. Esto hace que el entrenamiento sea incre칤blemente r치pido.

---
### **Parte 4: Entrenar y Evaluar el Modelo**
Con el modelo listo, el proceso de compilaci칩n y entrenamiento es el habitual. Deber칤as notar que cada 칠poca se completa mucho m치s r치pido que si entren치ramos una red similar desde cero.

```python
# Compilar el modelo
model.compile(
    optimizer=keras.optimizers.Adam(),
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=[keras.metrics.CategoricalAccuracy()]
)

# Entrenar el modelo
# Solo se entrenar치 la "cabeza" que a침adimos
epochs = 15
history = model.fit(
    x_train,
    y_train_cat,
    epochs=epochs,
    validation_data=(x_test, y_test_cat)
)

# Graficar los resultados
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['categorical_accuracy'], label='Training Accuracy')
plt.plot(history.history['val_categorical_accuracy'], label='Validation Accuracy')
plt.title('Accuracy a lo largo de las 칄pocas')
plt.legend()
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('P칠rdida (Loss) a lo largo de las 칄pocas')
plt.legend()
plt.grid(True)
plt.show()

# Evaluar el rendimiento final
final_loss, final_accuracy = model.evaluate(x_test, y_test_cat, verbose=0)
print(f"\nPrecisi칩n final en el conjunto de prueba: {final_accuracy * 100:.2f}%")
```

---
### **Preguntas para el An치lisis y Discusi칩n**
1.  **Rendimiento:** 쯈u칠 nivel de precisi칩n alcanzaste en el conjunto de validaci칩n? 쯊e parece un buen resultado considerando que solo entrenaste el modelo durante 15 칠pocas?
2.  **Eficiencia:** Compara el n칰mero de par치metros "Trainable" con los "Non-trainable" en el resumen del modelo. 쯈u칠 te dice esto sobre la eficiencia del Transfer Learning?
3.  **Global Average Pooling:** En nuestro clasificador, usamos una capa `GlobalAveragePooling2D` antes de la capa `Dense`. 쮺u치l es la alternativa a esta capa y por qu칠 `GlobalAveragePooling2D` es a menudo una mejor opci칩n en Transfer Learning? (Pista: `Flatten`).
4.  **Pr칩ximos Pasos:** Si quisieras mejorar a칰n m치s el rendimiento de este modelo y tuvieras un poco m치s de datos, 쯖u치l ser칤a el siguiente paso l칩gico a seguir? (Pista: Piensa en la segunda estrategia de Transfer Learning).