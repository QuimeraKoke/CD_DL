## Laboratorio Práctico: Transfer Learning con Extracción de Características

### **Objetivo del Laboratorio**
* Implementar la técnica de **extracción de características** usando un modelo pre-entrenado (EfficientNetV2).
* Cargar un modelo convolucional (EfficientNetV2), **congelar** sus pesos y añadirle un nuevo clasificador.
* Entrenar el nuevo clasificador en un dataset diferente (CIFAR-10) y lograr una alta precisión con muy poco tiempo de entrenamiento.
* Entender en la práctica los beneficios de no empezar desde cero.

### **Entorno**
Este laboratorio utiliza **Python**, **TensorFlow** y **Keras**.

---
### **Parte 0: Configuración y Preparación del Dataset**
Comenzamos con la configuración habitual, importando librerías y preparando el dataset CIFAR-10.

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt

# Cargar el dataset CIFAR-10
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()

# Normalizar los valores de los píxeles
# EfficientNetV2 no requiere normalización a [0, 1],
# ya que su capa de preprocesamiento interna se encarga de ello.
# Solo nos aseguramos de que el tipo de dato sea correcto.
x_train = x_train.astype("float32")
x_test = x_test.astype("float32")

# Convertir las etiquetas a formato one-hot encoding
num_classes = 10
y_train_cat = keras.utils.to_categorical(y_train, num_classes)
y_test_cat = keras.utils.to_categorical(y_test, num_classes)

print(f"Forma de x_train: {x_train.shape}")
print(f"Forma de y_train (categórica): {y_train_cat.shape}")
```

---
### **Parte 1: Cargar el Modelo Pre-entrenado (La Base Convolucional)**
Cargaremos `EfficientNetV2B0`, un modelo moderno y muy eficiente. La clave aquí es usar dos argumentos importantes:
* `weights='imagenet'`: Para cargar los pesos ya entrenados.
* `include_top=False`: Para **descartar** la capa de clasificación original de ImageNet (que tenía 1000 clases).
* `input_shape`: Para adaptar el modelo a nuestras imágenes de 32x32x3.

```python
# Definir la forma de entrada
INPUT_SHAPE = (32, 32, 3)

# Cargar la base del modelo EfficientNetV2B0
base_model = keras.applications.EfficientNetV2B0(
    include_top=False, # No incluir el clasificador original
    weights='imagenet',
    input_shape=INPUT_SHAPE
)

print("Modelo base cargado exitosamente.")
```
---
### **Parte 2: Congelar la Base Convolucional 🧊**
Este es el paso más importante en la extracción de características. Le decimos a Keras que **no queremos re-entrenar** la base convolucional; solo la usaremos para extraer características. Sus pesos deben permanecer "congelados".

```python
# Congelar la base del modelo
base_model.trainable = False

print("La base del modelo ha sido congelada.")

# Verifiquemos el estado
# base_model.summary() # Descomentar para ver el gran número de parámetros no entrenables
```
---
### **Parte 3: Construir el Nuevo Modelo con Nuestra Propia "Cabeza"**
Ahora, construiremos un nuevo modelo `Sequential` que usará la base congelada como su primera capa y le añadiremos nuestro propio clasificador (la "cabeza") al final.

```python
# Construir el modelo final
model = keras.Sequential([
    # 1. La base convolucional congelada
    base_model,
    
    # 2. Capa de pooling para aplanar los mapas de características
    layers.GlobalAveragePooling2D(),
    
    # 3. Capa de Dropout para regularización
    layers.Dropout(0.3),
    
    # 4. Nuestra capa de clasificación final
    layers.Dense(num_classes, activation="softmax")
], name="transfer_learning_model")


# Imprimir el resumen del nuevo modelo
# Observa la diferencia entre "Total params" y "Trainable params"
model.summary()
```
**Observación Clave:** Fíjate en el resumen del modelo. Verás millones de parámetros, ¡pero la mayoría son "Non-trainable"! Solo entrenaremos los parámetros de la nueva capa `Dense`, que son muy pocos. Esto hace que el entrenamiento sea increíblemente rápido.

---
### **Parte 4: Entrenar y Evaluar el Modelo**
Con el modelo listo, el proceso de compilación y entrenamiento es el habitual. Deberías notar que cada época se completa mucho más rápido que si entrenáramos una red similar desde cero.

```python
# Compilar el modelo
model.compile(
    optimizer=keras.optimizers.Adam(),
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=[keras.metrics.CategoricalAccuracy()]
)

# Entrenar el modelo
# Solo se entrenará la "cabeza" que añadimos
epochs = 15
history = model.fit(
    x_train,
    y_train_cat,
    epochs=epochs,
    validation_data=(x_test, y_test_cat)
)

# Graficar los resultados
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['categorical_accuracy'], label='Training Accuracy')
plt.plot(history.history['val_categorical_accuracy'], label='Validation Accuracy')
plt.title('Accuracy a lo largo de las Épocas')
plt.legend()
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Pérdida (Loss) a lo largo de las Épocas')
plt.legend()
plt.grid(True)
plt.show()

# Evaluar el rendimiento final
final_loss, final_accuracy = model.evaluate(x_test, y_test_cat, verbose=0)
print(f"\nPrecisión final en el conjunto de prueba: {final_accuracy * 100:.2f}%")
```

---
### **Preguntas para el Análisis y Discusión**
1.  **Rendimiento:** ¿Qué nivel de precisión alcanzaste en el conjunto de validación? ¿Te parece un buen resultado considerando que solo entrenaste el modelo durante 15 épocas?
2.  **Eficiencia:** Compara el número de parámetros "Trainable" con los "Non-trainable" en el resumen del modelo. ¿Qué te dice esto sobre la eficiencia del Transfer Learning?
3.  **Global Average Pooling:** En nuestro clasificador, usamos una capa `GlobalAveragePooling2D` antes de la capa `Dense`. ¿Cuál es la alternativa a esta capa y por qué `GlobalAveragePooling2D` es a menudo una mejor opción en Transfer Learning? (Pista: `Flatten`).
4.  **Próximos Pasos:** Si quisieras mejorar aún más el rendimiento de este modelo y tuvieras un poco más de datos, ¿cuál sería el siguiente paso lógico a seguir? (Pista: Piensa en la segunda estrategia de Transfer Learning).