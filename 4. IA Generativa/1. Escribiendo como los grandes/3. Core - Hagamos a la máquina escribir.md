
### **Instrucciones**

#### **Generación de Texto Controlada con GPT (Evaluación)**

##### **Descripción**
En esta evaluación, utilizarás un modelo de lenguaje generativo pre-entrenado de la familia GPT (específicamente, GPT-2) para generar texto coherente. El foco principal no será solo generar texto, sino aprender a **controlar y guiar el proceso de generación**. Para ello, experimentarás con diferentes parámetros de decodificación (como `temperature` y `top_k`) y aplicarás técnicas básicas de **Prompt Engineering** para instruir al modelo a realizar tareas específicas sin necesidad de re-entrenamiento.

##### **Objetivo**
El objetivo de esta actividad es que demuestres tu comprensión sobre cómo interactuar con un LLM generativo. Deberás:
* Cargar un modelo GPT-2 pre-entrenado y su tokenizador desde la librería `transformers`.
* Implementar un pipeline básico para generar texto a partir de un prompt inicial.
* Experimentar y analizar cómo los parámetros `temperature`, `top_k` y `max_length` afectan el estilo, la coherencia y la creatividad del texto generado.
* Aplicar técnicas de prompting **zero-shot** y **few-shot** para guiar al modelo a completar una tarea simple.

##### **Instrucciones**

1.  **Carga de Modelo y Tokenizador:**
    * Instala las librerías necesarias, principalmente `transformers` y `tensorflow`.
    * Carga el modelo `TFGPT2LMHeadModel` y el tokenizador `GPT2Tokenizer` desde Hugging Face. El modelo base `gpt2` es suficiente para esta tarea.

2.  **Generación de Texto Básica (Línea Base):**
    * Define un prompt inicial (ej. "En un futuro lejano, la humanidad descubrió...").
    * Tokeniza el prompt para convertirlo en un formato que el modelo entienda.
    * Utiliza el método `model.generate()` para generar una continuación del texto con los parámetros por defecto.
    * Decodifica la salida para convertir los tokens de vuelta a texto legible. Este será tu resultado base.

3.  **Experimentación con Parámetros de Generación:**
    * Utilizando el **mismo prompt inicial**, genera al menos tres textos adicionales, cada uno modificando un parámetro clave:
        * **Texto 1 (Baja Temperatura):** Genera un texto con una `temperature` baja (ej. `0.7`).
        * **Texto 2 (Alta Temperatura):** Genera un texto con una `temperature` alta (ej. `1.5`).
        * **Texto 3 (Top-k Sampling):** Genera un texto utilizando el parámetro `top_k` (ej. `top_k=50`).

4.  **Aplicación de Prompt Engineering:**
    * Elige una tarea simple, como "escribir un eslogan para un producto".
    * **Prompt Zero-shot:** Escribe un prompt que le pida directamente al modelo que complete la tarea. (ej. "Escribe un eslogan para una nueva bebida energética llamada 'Volt'").
    * **Prompt Few-shot:** Diseña un prompt que incluya 2 o 3 ejemplos completos de la tarea antes de hacer la solicitud final, para mostrarle al modelo el formato y estilo que esperas.

5.  **Presentación y Análisis de Resultados:**
    * Presenta de forma clara todos los textos generados, indicando qué prompt y qué parámetros se utilizaron para cada uno.
    * Escribe un breve análisis comparativo para la sección 3. Explica cómo la temperatura y el top-k sampling afectaron la calidad y el estilo del texto.
    * Compara los resultados de los prompts zero-shot y few-shot. ¿Mejoró la calidad del eslogan al darle ejemplos al modelo? ¿Por qué?

##### **Sugerencias**

* El método clave que usarás es `model.generate()`. Investiga sus argumentos en la documentación de Hugging Face.
* Recuerda que el tokenizador debe devolver los tensores en formato TensorFlow (`return_tensors='tf'`).
* Para empezar, usa un valor de `max_length` moderado (ej. `150`) para que la generación sea rápida.
* El proceso siempre es: **Texto -> Tokenizar -> Generar IDs -> Decodificar -> Texto**.
* Al decodificar, puedes usar la opción `skip_special_tokens=True` para obtener un texto más limpio.

---
## Laboratorio Práctico: Generación de Texto Controlada con GPT

### **Objetivo del Laboratorio**
* Implementar un pipeline de generación de texto usando un modelo GPT-2 pre-entrenado.
* Experimentar con los parámetros de decodificación para controlar la creatividad y coherencia del texto.
* Aplicar técnicas de prompt engineering para guiar al modelo hacia tareas específicas.

### **Entorno**
Este laboratorio requiere **Python**, **TensorFlow**, y la librería `transformers`.

---
### **Parte 0: Instalación y Configuración**

```python
# Instalar la librería de Hugging Face
!pip install transformers -q

import tensorflow as tf
from transformers import TFGPT2LMHeadModel, GPT2Tokenizer

print("Librerías importadas y listas.")
```
---
### **Parte 1: Cargar el Modelo y Tokenizador GPT-2**
Cargaremos GPT-2, un potente modelo generativo de OpenAI que es lo suficientemente pequeño para ejecutarse en entornos como Google Colab.

```python
# Cargar el tokenizador y el modelo pre-entrenado
model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = TFGPT2LMHeadModel.from_pretrained(model_name)

# GPT-2 no tiene un token de padding por defecto, así que asignamos el token de fin de secuencia
tokenizer.pad_token = tokenizer.eos_token

print("Modelo GPT-2 y tokenizador cargados.")
```

---
### **Parte 2: Generación de Texto Básica**
Vamos a generar texto a partir de un prompt simple con los parámetros por defecto para tener una línea base.

```python
# Prompt inicial
prompt = "In a distant future, humanity discovered a new planet similar to Earth"

# 1. Tokenizar el prompt
input_ids = tokenizer.encode(prompt, return_tensors='tf')

# 2. Generar la continuación del texto
# `model.generate` es la función clave aquí
output_ids = model.generate(
    input_ids,
    max_length=100, # Longitud máxima de la secuencia de salida
    num_return_sequences=1
)

# 3. Decodificar el resultado
baseline_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

print("--- TEXTO BASE (PARÁMETROS POR DEFECTO) ---")
print(baseline_text)
```
---
### **Parte 3: Explorando Parámetros de Generación**
Ahora, usando el mismo prompt, veremos cómo cambia el resultado al ajustar `temperature` y `top_k`.

#### **A. El Efecto de `temperature`**
```python
# Baja temperatura: texto más predecible y coherente
output_low_temp = model.generate(
    input_ids,
    max_length=100,
    temperature=0.7,
    do_sample=True # Activar el muestreo
)
text_low_temp = tokenizer.decode(output_low_temp[0], skip_special_tokens=True)
print("\n--- TEXTO CON TEMPERATURA BAJA (0.7) ---")
print(text_low_temp)

# Alta temperatura: texto más creativo y arriesgado
output_high_temp = model.generate(
    input_ids,
    max_length=100,
    temperature=1.5,
    do_sample=True
)
text_high_temp = tokenizer.decode(output_high_temp[0], skip_special_tokens=True)
print("\n--- TEXTO CON TEMPERATURA ALTA (1.5) ---")
print(text_high_temp)
```

#### **B. El Efecto de `top_k` Sampling**
`top_k` limita el muestreo a las 'k' palabras más probables, evitando opciones extrañas.

```python
# Top-k sampling: limita el vocabulario de donde se muestrea
output_top_k = model.generate(
    input_ids,
    max_length=100,
    top_k=50, # Solo considerar las 50 palabras más probables en cada paso
    do_sample=True
)
text_top_k = tokenizer.decode(output_top_k[0], skip_special_tokens=True)
print("\n--- TEXTO CON TOP-K SAMPLING (k=50) ---")
print(text_top_k)
```
---
### **Parte 4: El Poder del Prompt Engineering**
Veamos cómo podemos guiar al modelo para una tarea específica sin re-entrenarlo.

#### **A. Prompt Zero-Shot**
Le pedimos directamente al modelo que haga algo.

```python
prompt_zero_shot = "Escribe un eslogan para una nueva marca de café premium llamada 'Aura Café'."
input_ids_zs = tokenizer.encode(prompt_zero_shot, return_tensors='tf')

output_zs = model.generate(
    input_ids_zs,
    max_length=50,
    temperature=0.8,
    top_k=50,
    do_sample=True
)
text_zs = tokenizer.decode(output_zs[0], skip_special_tokens=True)

print("\n--- PROMPT ZERO-SHOT ---")
print(text_zs)
```

#### **B. Prompt Few-Shot**
Le damos al modelo ejemplos para mostrarle el formato y estilo que queremos.

```python
prompt_few_shot = """
Escribe un eslogan creativo para cada producto.

Producto: Coche eléctrico "Volt"
Eslogan: El futuro es silencioso.

Producto: Reloj de lujo "Eternitas"
Eslogan: El tiempo en tus manos.

Producto: Zapatillas para correr "Impulse"
Eslogan: Siente el impulso en cada paso.

Producto: Café premium "Aura Café"
Eslogan:
"""

input_ids_fs = tokenizer.encode(prompt_few_shot, return_tensors='tf')

output_fs = model.generate(
    input_ids_fs,
    max_length=len(input_ids_fs[0]) + 10, # Generar solo un poco más de texto
    temperature=0.7,
    top_k=50,
    do_sample=True
)
text_fs = tokenizer.decode(output_fs[0], skip_special_tokens=True)

print("\n--- PROMPT FEW-SHOT ---")
print(text_fs)
```

**Análisis:**
Generalmente, el prompt **Few-Shot** produce resultados de mayor calidad y más alineados con el formato deseado, ya que los ejemplos guían al modelo de manera mucho más efectiva que una simple instrucción. Este laboratorio demuestra que la forma en que "hablamos" con un LLM es crucial para obtener los resultados que buscamos.