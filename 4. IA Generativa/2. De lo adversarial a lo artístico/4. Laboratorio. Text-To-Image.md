## Generación de Imágenes a partir de Texto con Stable Diffusion

### **Objetivo del Laboratorio**
* Utilizar un modelo de difusión pre-entrenado de vanguardia (Stable Diffusion) para generar imágenes a partir de descripciones textuales (prompts).
* Aprender a usar la librería `diffusers` de Hugging Face, la herramienta estándar para trabajar con estos modelos.
* Experimentar con diferentes prompts y parámetros para entender cómo guiar y controlar el proceso creativo de la IA.

### **Entorno y Requisitos**
Este laboratorio es computacionalmente intensivo. **Es altamente recomendable ejecutarlo en un entorno con acceso a una GPU** (como Google Colab o Kaggle). Sin una GPU, el proceso de generación puede ser extremadamente lento.

---
### **Parte 0: Instalación y Configuración**
Primero, necesitamos instalar las librerías necesarias de Hugging Face.

```python
# Instalar las librerías requeridas
!pip install diffusers transformers accelerate safetensors -q

import torch
from diffusers import StableDiffusionPipeline
import matplotlib.pyplot as plt

print("Librerías instaladas y listas.")
```

---
### **Parte 1: Cargar el Pipeline de Stable Diffusion**
La librería `diffusers` simplifica enormemente el uso de estos modelos complejos a través de un objeto llamado `Pipeline`. Este objeto se encarga de cargar todos los componentes necesarios (el codificador de texto, el modelo U-Net de difusión, el VAE, etc.).

Cargaremos una versión popular y eficiente de Stable Diffusion.

```python
# Definir el ID del modelo pre-entrenado que vamos a usar
model_id = "stabilityai/stable-diffusion-2-1-base"

# Cargar el pipeline. Usamos float16 para ahorrar memoria en la GPU.
# Esto descargará varios gigabytes de pesos la primera vez que se ejecute.
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)

# Mover el pipeline a la GPU para una generación rápida
device = "cuda" if torch.cuda.is_available() else "cpu"
pipe = pipe.to(device)

print(f"Pipeline cargado y movido al dispositivo: {device}")
```
---
### **Parte 2: Nuestra Primera Generación de Imágenes**
Ahora, la parte emocionante. Vamos a darle a nuestro modelo un "prompt" (una descripción de texto) y veremos qué crea.

```python
# Definir nuestro prompt
prompt = "A high-quality photo of an astronaut riding a horse on Mars"

# Generar la imagen. La salida es un objeto que contiene la imagen generada.
# El pipeline se encarga de todo el proceso iterativo de denoising.
output = pipe(prompt)
image = output.images[0] # Extraemos la primera (y única) imagen generada

# Mostrar la imagen
plt.figure(figsize=(8, 8))
plt.imshow(image)
plt.axis('off')
plt.title(f"Prompt: '{prompt}'")
plt.show()
```
¡Felicidades! Acabas de crear tu primera imagen con un modelo de difusión. Ahora, exploremos cómo podemos mejorar y controlar el resultado.

---
### **Parte 3: Experimentando con Prompts y Parámetros**

La calidad de la imagen depende enormemente de la calidad del prompt y de algunos parámetros clave.

#### **A. El Arte del Prompt Positivo y Negativo**
Un buen prompt es detallado. Añadir palabras clave como "hyperrealistic", "4k", "detailed", o nombrar estilos de artistas puede cambiar drásticamente el resultado.

Igual de importante es el **prompt negativo**, donde le decimos al modelo qué cosas queremos evitar.

```python
# Un prompt más detallado y un prompt negativo
positive_prompt = "A majestic lion king on a rock, dramatic lighting, high detail, photorealistic, style of national geographic"
negative_prompt = "cartoon, drawing, low quality, ugly, blurry, text, watermark"

# Generar la imagen con ambos prompts
image_detailed = pipe(prompt=positive_prompt, negative_prompt=negative_prompt).images[0]

# Mostrar la imagen mejorada
plt.figure(figsize=(8, 8))
plt.imshow(image_detailed)
plt.axis('off')
plt.title("Generación con Prompts Detallados")
plt.show()
```

#### **B. Controlando el Proceso con Parámetros**

* **`num_inference_steps`**: El número de pasos de "denoising". Más pasos pueden dar más detalle, pero tardan más. Un buen rango es 25-50.
* **`guidance_scale`**: Un valor que indica qué tan estrictamente debe seguir el modelo el prompt de texto. Un valor bajo (ej. 2-5) le da más libertad creativa. Un valor alto (ej. 7-12) lo fuerza a adherirse más al prompt.
* **`generator` (Seed)**: Para obtener resultados reproducibles. Si usas la misma semilla (`seed`), obtendrás la misma imagen para el mismo prompt y parámetros.

Veamos un ejemplo combinando todo.

```python
# Definir un generador con una semilla fija para reproducibilidad
seed = 42
generator = torch.Generator(device=device).manual_seed(seed)

# Definir el prompt
prompt = "A beautiful castle in the Swiss Alps during winter, fantasy art, detailed, epic scale"

# Generar la imagen controlando todos los parámetros
final_image = pipe(
    prompt=prompt,
    negative_prompt="low quality, blurry, people",
    num_inference_steps=50,
    guidance_scale=9,
    generator=generator # Usamos el generador con la semilla fija
).images[0]


# Mostrar la imagen final
plt.figure(figsize=(8, 8))
plt.imshow(final_image)
plt.axis('off')
plt.title("Generación Controlada con Parámetros")
plt.show()

```

---
### **Reto para los Estudiantes y Conclusión**

Ahora es tu turno de experimentar.
1.  **Prueba tus propios prompts:** Intenta ser lo más descriptivo posible. Mezcla conceptos, estilos artísticos ("in the style of Van Gogh"), tipos de cámara ("shot on a Canon 5D"), etc.
2.  **Juega con los parámetros:** Toma un prompt y una semilla fija. Genera la misma imagen cambiando solo el `guidance_scale` para ver cómo afecta al resultado. Haz lo mismo con `num_inference_steps`.
3.  **Combina ideas:** ¿Qué pasa si pides "A photo of a cat programming a neural network, cyberpunk city in the background"?

Este laboratorio demuestra cómo la IA generativa, y en particular los modelos de difusión, se han convertido en una herramienta creativa increíblemente potente y accesible. La habilidad ya no reside solo en el entrenamiento del modelo, sino en el arte de "dialogar" con él a través de prompts y parámetros para dar vida a una visión.