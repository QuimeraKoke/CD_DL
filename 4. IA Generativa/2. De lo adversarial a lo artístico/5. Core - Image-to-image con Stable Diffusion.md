
### **Instrucciones**

#### **Edición de Imágenes con IA: Image-to-Image con Stable Diffusion (Evaluación)**

##### **Descripción**
En esta evaluación, explorarás una de las capacidades más fascinantes de los modelos de difusión: la **traducción de imagen a imagen (Image-to-Image)**. A diferencia de la generación a partir de ruido puro, en esta tarea proporcionarás una **imagen de entrada** (como un boceto, un dibujo simple o una foto) junto con un **prompt de texto**. El modelo utilizará la composición y las formas de tu imagen como base, pero la transformará para que coincida con el estilo y el contenido descrito en tu prompt.

##### **Objetivo**
El objetivo principal es implementar y experimentar con el pipeline de Image-to-Image de Stable Diffusion. Deberás demostrar tu habilidad para:
* Cargar y utilizar un pipeline de difusión pre-entrenado para la tarea de Image-to-Image.
* Preparar una imagen de entrada y combinarla con un prompt de texto para guiar la generación.
* Controlar el proceso creativo ajustando parámetros clave, especialmente el parámetro `strength` (fuerza), que determina cuánto debe cambiar la imagen original.
* Analizar y presentar los resultados, comparando la imagen de entrada con las diversas salidas generadas.

##### **Instrucciones**

1.  **Carga de Librerías y Modelo:**
    * Instala y carga las librerías necesarias, incluyendo `diffusers`, `transformers` y `torch`.
    * Carga el pipeline pre-entrenado para la tarea de Image-to-Image. El modelo base `stabilityai/stable-diffusion-2-1` puede ser cargado en un pipeline de `StableDiffusionImg2ImgPipeline`.
    * Asegúrate de mover el pipeline a una GPU ("cuda") para un rendimiento adecuado.

2.  **Preparación de la Imagen de Entrada:**
    * Encuentra o crea una imagen de entrada. Puede ser un dibujo simple que hagas en un programa de edición, un boceto lineal, o una fotografía que quieras transformar.
    * Carga tu imagen de entrada en el entorno de trabajo (por ejemplo, súbela a tu notebook de Colab).
    * Utiliza la librería `PIL` (Pillow) para abrir la imagen y asegúrate de que esté en modo `RGB`.
    * Redimensiona la imagen a un tamaño compatible con el modelo (por ejemplo, 768x768 píxeles).

3.  **Generación Image-to-Image y Experimentación:**
    * Escribe un **prompt de texto** que describa el resultado final que deseas. Por ejemplo, si tu entrada es un boceto de un paisaje, tu prompt podría ser "A beautiful oil painting of a fantasy landscape, detailed, epic lighting".
    * Llama al pipeline pasándole como argumentos tanto el `prompt` como tu `image` de entrada.
    * **Experimenta con el parámetro `strength`**: Realiza varias generaciones de la misma imagen y prompt, pero con diferentes valores de `strength` (ej. `0.3`, `0.5`, `0.75`). Este parámetro controla cuánta importancia se le da a la imagen original frente al prompt.
    * Experimenta también con el parámetro `guidance_scale` para ajustar la adherencia al prompt.

4.  **Presentación y Análisis de Resultados:**
    * Crea una visualización que muestre claramente la **imagen original** al lado de al menos **tres imágenes generadas** con diferentes valores de `strength` o prompts.
    * Para cada imagen generada, escribe una breve descripción de los parámetros que usaste y cómo afectaron el resultado final.
    * En un párrafo final, discute cómo el parámetro `strength` te permite balancear entre mantener la estructura original y la creatividad guiada por el texto.

##### **Sugerencias**

* El pipeline que necesitas para esta tarea es `StableDiffusionImg2ImgPipeline` de la librería `diffusers`.
* El parámetro `strength` es un valor entre 0.0 y 1.0. Un valor bajo (ej. 0.2) hará cambios muy sutiles en la imagen original. Un valor alto (ej. 0.9) ignorará en gran medida la imagen original y se basará más en el prompt, casi como una generación de texto a imagen. Un buen punto de partida para experimentar es entre 0.6 y 0.8.
* Asegúrate de que tu imagen de entrada sea un objeto de la clase `PIL.Image`.
* Para obtener resultados reproducibles mientras experimentas, puedes crear un `torch.Generator` con una semilla (`seed`) fija y pasarlo al pipeline.

---
## Laboratorio Práctico: Edición de Imágenes con IA (Image-to-Image)

### **Objetivo del Laboratorio**
* Implementar el pipeline de Image-to-Image usando Stable Diffusion.
* Cargar una imagen base y transformarla usando prompts de texto.
* Entender y experimentar con el parámetro `strength` para controlar el nivel de transformación.

### **Entorno**
Este laboratorio requiere una **GPU**. Se recomienda ejecutarlo en Google Colab.

---
### **Parte 0: Instalación y Configuración**

```python
# Instalar las librerías necesarias
!pip install diffusers transformers accelerate safetensors -q

import torch
from diffusers import StableDiffusionImg2ImgPipeline
from PIL import Image
import requests
from io import BytesIO
import matplotlib.pyplot as plt

print("Librerías instaladas y listas.")
```

---
### **Parte 1: Cargar el Pipeline Image-to-Image**
Cargaremos un modelo pre-entrenado en un pipeline especializado para la tarea de imagen a imagen.

```python
# ID del modelo y carga del pipeline
model_id = "stabilityai/stable-diffusion-2-1"
pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, torch_dtype=torch.float16)

# Mover el pipeline a la GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
pipe = pipe.to(device)

print(f"Pipeline Img2Img cargado y movido al dispositivo: {device}")
```

---
### **Parte 2: Cargar y Preparar la Imagen de Entrada**
Para este ejemplo, descargaremos un boceto simple de un paisaje desde una URL. Puedes reemplazar esta URL por la de tu propia imagen o subir un archivo.

```python
# URL de una imagen de boceto simple
url = "[https://c.pxhere.com/photos/a3/64/photo-155259.jpg!d](https://c.pxhere.com/photos/a3/64/photo-155259.jpg!d)"

# Descargar y abrir la imagen
response = requests.get(url)
init_image = Image.open(BytesIO(response.content)).convert("RGB")

# Redimensionar la imagen a un tamaño compatible
init_image = init_image.resize((768, 512))

# Mostrar la imagen de entrada
plt.figure(figsize=(10, 6))
plt.imshow(init_image)
plt.title("Imagen de Entrada (Boceto)")
plt.axis('off')
plt.show()
```

---
### **Parte 3: La Primera Transformación - Boceto a Realidad**
Ahora, vamos a darle un prompt al modelo para que "coloree" y complete nuestro boceto.

```python
# Definir el prompt que describe el resultado deseado
prompt = "A beautiful mountain landscape at sunrise, detailed, fantasy art, epic, photorealistic"

# Usar una semilla para resultados reproducibles
generator = torch.Generator(device=device).manual_seed(1024)

# Generar la imagen. 'strength' controla cuánto se altera la imagen original.
# Un valor más alto significa más cambio.
output = pipe(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5, generator=generator)
image = output.images[0]

# Mostrar la imagen generada
plt.figure(figsize=(10, 6))
plt.imshow(image)
plt.title("Imagen Generada (strength=0.75)")
plt.axis('off')
plt.show()
```

---
### **Parte 4: Experimentando con el Parámetro `strength`**
El parámetro `strength` es clave en Image-to-Image. Veamos cómo diferentes valores afectan el resultado, manteniendo el mismo prompt y la misma semilla.

```python
# Lista de valores de strength para probar
strengths = [0.3, 0.5, 0.8]
generated_images = []

for s in strengths:
    generator = torch.Generator(device=device).manual_seed(1024)
    result_image = pipe(prompt=prompt, image=init_image, strength=s, guidance_scale=7.5, generator=generator).images[0]
    generated_images.append(result_image)

# Crear una visualización comparativa
fig, axs = plt.subplots(1, len(strengths) + 1, figsize=(20, 5))
fig.suptitle("Efecto del Parámetro 'strength'", fontsize=16)

# Mostrar la imagen original
axs[0].imshow(init_image)
axs[0].set_title("Original")
axs[0].axis('off')

# Mostrar las imágenes generadas
for i, s in enumerate(strengths):
    axs[i+1].imshow(generated_images[i])
    axs[i+1].set_title(f"strength={s}")
    axs[i+1].axis('off')

plt.show()
```

**Análisis:**
* Con `strength=0.3`, la imagen resultante es muy fiel al boceto original, con cambios sutiles de color y textura.
* Con `strength=0.5`, el modelo empieza a tomar más libertades creativas, añadiendo detalles que no estaban en el boceto.
* Con `strength=0.8`, la composición del boceto se respeta, pero el modelo ha reinventado casi por completo la escena (el estilo, la iluminación, los detalles), basándose mucho más en el prompt.

Este laboratorio muestra cómo la IA generativa no solo crea desde cero, sino que también puede ser una poderosa herramienta de colaboración y edición, transformando nuestras ideas iniciales en obras de arte detalladas.