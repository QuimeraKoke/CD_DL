## La Arquitectura Transformer - El Motor de los LLMs ⚙️

### **La Idea Clave - Mecanismos de Auto-Atención (Self-Attention)**

**Objetivo de la Clase:**
* Entender el concepto de auto-atención como el mecanismo que permite a un modelo comprender el contexto.
* Desglosar intuitivamente cómo funciona la auto-atención a través del concepto de Consultas, Claves y Valores (Queries, Keys, Values).
* Apreciar por qué este mecanismo es tan poderoso para manejar las relaciones entre palabras en una secuencia.

---

### **1. La Solución al Problema Secuencial: Creando Contexto en Paralelo**

En la clase anterior, vimos que las RNNs, aunque fueron la primera gran solución para crear representaciones contextuales, sufrían de dos grandes males: eran **lentas** por su naturaleza secuencial y tenían **dificultades con las dependencias a larga distancia**.

El Transformer se deshace de la recurrencia y propone una forma completamente nueva de construir contexto. En lugar de procesar palabra por palabra, se pregunta:

> ¿Podemos crear una representación contextual para cada palabra mirando todas las demás palabras de la oración **al mismo tiempo**?

La respuesta es sí, y el mecanismo para lograrlo es la **auto-atención**. Es el motor que permite a cada palabra conectarse directamente con cualquier otra, sin importar la distancia, para decidir qué información es relevante para definir su significado en *esta oración específica*.

![Arquitectura de un transformer](imgs/transformer-arch.png)

### **2. El Concepto: Cada Palabra "Pregunta" por su Contexto**

La auto-atención (Self-Attention) es un mecanismo que permite a cada palabra de una secuencia "mirar" a todas las demás palabras de la misma secuencia y decidir cuáles son las más importantes para entender su propio significado en ese contexto particular.

**La Analogía Clave:**

Consideremos la oración:

> "El **robot** dejó caer la **pelota** porque **él** estaba dañado"

Cuando el modelo procesa la palabra "**él**", necesita resolver una ambigüedad: ¿a quién se refiere "él"? ¿Al robot o a la pelota?

El mecanismo de auto-atención permite que la palabra "**él**" emita una especie de "pregunta" al resto de la oración. La pregunta podría ser algo como: *"Estoy buscando un sustantivo masculino singular en esta oración con el que pueda relacionarme"*.

Luego, "él" calcula una "puntuación de atención" o de relevancia con todas las demás palabras:
* La puntuación entre "**él**" y "**robot**" será **muy alta**, porque "robot" encaja con la descripción.
* La puntuación entre "**él**" y "**pelota**" será **muy baja**, porque "pelota" es un sustantivo femenino.

Como resultado, la representación final del pronombre "él" estará fuertemente influenciada por la representación de "robot", y el modelo "entenderá" que él es quien estaba dañado.

### **3. El Mecanismo: Queries, Keys y Values (Q, K, V)**

Para implementar esta idea, el Transformer utiliza una poderosa analogía de sistemas de recuperación de información. Para cada palabra de entrada (representada por su embedding), el modelo aprende a generar tres vectores distintos a través de matrices de pesos que se entrenan:

1.  **Query (Consulta, Q):** Representa la "pregunta" que una palabra está haciendo para encontrar contexto. Es el vector que busca activamente información. En nuestro ejemplo, es el vector de "**él**" buscando su antecedente.
2.  **Key (Clave, K):** Representa la "etiqueta" o el "índice" de una palabra. Anuncia las propiedades de esa palabra para que otras puedan encontrarla. El vector Key de "**robot**" esencialmente dice: "Soy un sustantivo masculino, soy un agente que puede actuar, etc.".
3.  **Value (Valor, V):** Representa el contenido real o el significado de la palabra. Es la información que se entregará si la palabra es considerada relevante.

**El Proceso en 4 Pasos:**

1.  **Cálculo de Puntuaciones:** Para una palabra que está "preguntando" (la Query), se calcula su puntuación de atención con todas las demás palabras de la oración. Esto se hace calculando el producto punto entre su vector **Query** y el vector **Key** de cada una de las otras palabras. `Puntuación = Q · K`
2.  **(Opcional) Escalar:** Las puntuaciones se dividen por la raíz cuadrada de la dimensión de los vectores Key para estabilizar los gradientes.
3.  **Normalización con Softmax:** Las puntuaciones se pasan a través de una función Softmax. Esto convierte las puntuaciones en un conjunto de pesos de atención positivos que suman 1. Ahora tenemos una distribución de probabilidad que nos dice a qué palabras prestarle más atención. (ej. `robot: 0.85`, `pelota: 0.03`, `dejó: 0.01`, ...).
4.  **Creación de la Salida:** La salida final para nuestra palabra original es una suma ponderada de **todos los vectores Value** de la oración. Los pesos utilizados para esta suma son los pesos de atención que calculamos en el paso 3.

Por ejemplo, para la frase: "Tom didn't come to work today because he was sick" el calculo de la salida para la palabra `sick` podemos ver que se tiene el vector `Value` con un mayor valor para Tom y He :

![Ejemplo Mapa de Atención](imgs/attention map ex.png)

Esto quiere decir que la palabra `sick` ha absorbido el contexto de su antecedente.

Para ver más ejemplos de transformer y atención se recomienda vistiar el siguiente [link](https://huggingface.co/spaces/exbert-project/exbert) 

### **4. Conclusión**

La auto-atención es un mecanismo elegante que logra tres cosas a la vez:
* Crea **representaciones profundamente contextuales** para cada token.
* Resuelve las **dependencias a larga distancia** sin esfuerzo, ya que cada palabra se conecta directamente con todas las demás.
* Es **altamente paralelizable**, ya que todo el proceso puede expresarse como multiplicaciones de matrices, lo que lo hace perfecto para las GPUs modernas.

Este mecanismo es la razón por la cual los Transformers pueden procesar texto de manera tan efectiva y eficiente, convirtiéndose en el pilar de los LLMs.