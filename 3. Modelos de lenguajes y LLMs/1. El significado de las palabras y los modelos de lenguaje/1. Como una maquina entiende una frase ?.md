## Más Allá de las RNNs: La Necesidad de un Nuevo Enfoque

**Objetivo de la Clase:**
* Conectar la limitación de los embeddings estáticos con la aparición de las RNNs como solución contextual.
* Comprender las limitaciones fundamentales de las arquitecturas secuenciales como las RNNs y LSTMs.
* Reconocer por qué estas limitaciones representaban un obstáculo para el avance del NLP.
* Introducir el paper "Attention Is All You Need" como el punto de inflexión que propuso una solución radical a estos problemas.

---

### **1. La Primera Solución al Contexto: Redes Neuronales Recurrentes (RNNs)**

En la clase anterior, vimos que los embeddings estáticos como Word2Vec, aunque revolucionarios, tienen una gran debilidad: no pueden diferenciar el significado de una palabra según su contexto. La palabra "banco" tiene el mismo vector siempre.

Para resolver este problema, la comunidad de NLP recurrió a las **Redes Neuronales Recurrentes (RNNs)** y sus variantes más potentes como las **LSTMs** y **GRUs**.

La idea era brillante: en lugar de analizar las palabras de forma aislada, se procesaría la secuencia de embeddings palabra por palabra. En cada paso, la RNN toma el embedding de la palabra actual y el "estado oculto" (la memoria) del paso anterior para producir un nuevo estado oculto.

![Diagrama Conceptual de una RNN](/imgs/rnn.png)

De esta forma, el estado oculto en cada punto de la secuencia es una representación que **contiene información de todas las palabras que la precedieron**. ¡Habíamos logrado crear representaciones contextuales! Durante muchos años, esta fue la arquitectura de elección para casi todas las tareas de NLP.

Sin embargo, a medida que los modelos y los datasets crecían, dos limitaciones fundamentales de este enfoque secuencial se hicieron evidentes.

---

### **2. Los Muros de la Recurrencia: Dos Problemas Clave**

#### **Problema 1: La Falta de Paralelización (Lentitud)** ⏳
La naturaleza secuencial de las RNNs es su mayor debilidad computacional. Para procesar la palabra número 100 de una oración, el modelo **debe** haber procesado primero las 99 palabras anteriores, ya que la salida de la celda 99 es una entrada para la celda 100.

* **Consecuencia:** Es imposible paralelizar el procesamiento de una misma secuencia. No podemos calcular el estado de todas las palabras al mismo tiempo. En la era de las GPUs, que son expertas en realizar miles de cálculos en paralelo, esta dependencia secuencial se convirtió en un cuello de botella masivo, haciendo que el entrenamiento en datasets gigantescos fuera extremadamente lento.

#### **Problema 2: Las Dependencias a Larga Distancia** ↔️
Aunque las LSTMs y GRUs fueron diseñadas para "recordar" información a largo plazo y mitigar el problema de la desaparición de gradientes, todavía tienen dificultades con dependencias muy largas.

* **Consecuencia:** En un párrafo largo, la información del principio (el "contexto") se va "diluyendo" a medida que pasa a través de muchas celdas recurrentes. Para cuando el modelo llega al final del párrafo, la "memoria" de las primeras palabras puede ser débil o imprecisa.

    * **Ejemplo:** "Crecí en un pequeño pueblo en los Alpes suizos, rodeado de montañas y lagos. Disfruté de la naturaleza y aprendí mucho de mis abuelos, quienes me enseñaron sobre la flora y fauna local. Después de muchos años viajando por el mundo y viviendo en diferentes países, todavía puedo hablar fluidamente **[???]**."

    Para que el modelo prediga "alemán", "francés" o "italiano", necesita recordar con fuerza la palabra "suizos" del principio. En una RNN, esa información tiene que sobrevivir un largo viaje a través de muchas palabras intermedias.

---

### **3. La Solución: "Attention Is All You Need"** ⚡

En 2017, un paper de investigadores de Google titulado **"Attention Is All You Need"** cambió el curso del NLP para siempre. Su propuesta fue radical y elegante.

**La Propuesta Central:**
> Deshagámonos por completo de la recurrencia. Construyamos una arquitectura que pueda procesar todas las palabras de una secuencia **simultáneamente**.

**¿Cómo es posible sin perder el orden?**
La solución fue basar toda la arquitectura en un único y potente mecanismo llamado **atención (attention)**, específicamente la **auto-atención (self-attention)**.

* **¿Qué hace la atención?** Permite que cada palabra en la oración se conecte **directamente** con todas las demás palabras. Al procesar una palabra, el modelo puede "prestar atención" a cualquier otra palabra de la secuencia, sin importar lo lejos que esté, y extraer contexto de ella.

**Los Beneficios Inmediatos:**

1.  **Paralelización Masiva:** Al no haber dependencias secuenciales, los cálculos para todas las palabras se pueden realizar en paralelo, aprovechando al máximo la potencia de las GPUs y reduciendo drásticamente los tiempos de entrenamiento.
2.  **Dependencias a Larga Distancia Resueltas:** El camino entre dos palabras cualquiera en la secuencia es ahora de longitud 1 (una sola operación de atención). El problema de la distancia desaparece.

Esta nueva arquitectura, llamada **Transformer**, no solo resolvió los problemas de las RNNs, sino que lo hizo con una eficacia y un rendimiento superiores, sentando las bases para la era de los Modelos de Lenguaje de Gran Escala (LLM) como BERT y GPT.

En la siguiente clase, nos sumergiremos en el corazón de esta revolución: el mecanismo de auto-atención.