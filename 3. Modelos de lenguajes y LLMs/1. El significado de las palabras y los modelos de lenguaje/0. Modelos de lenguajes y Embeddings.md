## Fundamentos de NLP - Modelos de Lenguaje y Word Embeddings

**Objetivo de la Clase:**
* Entender el desaf√≠o fundamental de representar texto en un formato que las m√°quinas puedan procesar.
* Describir las limitaciones del enfoque b√°sico de "One-Hot Encoding".
* Introducir el concepto de **Word Embeddings** como una representaci√≥n densa y significativa de las palabras.
* Conocer **Word2Vec** como un modelo pionero en el aprendizaje de embeddings y entender su limitaci√≥n clave (la falta de contexto).

---
### **1. El Desaf√≠o: ¬øC√≥mo le Damos Sentido a las Palabras?**

Las redes neuronales son incre√≠blemente buenas procesando n√∫meros, pero no entienden palabras como "rey" o "reina". El primer paso en cualquier tarea de Procesamiento del Lenguaje Natural (NLP) es convertir el texto en vectores num√©ricos.

**El Primer Intento: One-Hot Encoding**

La forma m√°s simple de hacer esto es crear un vector para cada palabra que tenga el tama√±o de todo nuestro vocabulario. El vector est√° lleno de ceros, excepto por un "1" en la posici√≥n que corresponde a esa palabra.

* Vocabulario: `["hola", "adi√≥s", "gato", "perro"]` (tama√±o 4)
* `hola`: `[1, 0, 0, 0]`
* `gato`: `[0, 0, 1, 0]`

**Problemas del One-Hot Encoding:**
1.  **Dimensionalidad Enorme:** Si nuestro vocabulario tiene 50,000 palabras, cada vector tendr√° 50,000 dimensiones. ¬°Es computacionalmente muy ineficiente!
2.  **Vectores Dispersos (Sparse):** La mayor√≠a de los valores son cero.
3.  **Falta de Relaci√≥n Sem√°ntica:** Lo m√°s importante es que este m√©todo no captura ninguna relaci√≥n de significado entre las palabras. El vector de "rey" es tan diferente del de "reina" como lo es del de "manzana". Matem√°ticamente, son ortogonales, lo que implica que no tienen ninguna relaci√≥n.

---
### **2. La Soluci√≥n: Representaciones Distribuidas (Word Embeddings)** üß†

Para resolver esto, surgieron los **Word Embeddings** (incrustaciones de palabras). La idea es revolucionaria:

> En lugar de usar vectores enormes y dispersos, representaremos cada palabra como un **vector denso, de baja dimensionalidad** (ej. 100, 200 o 300 dimensiones) y con valores de punto flotante.

El objetivo es que en este nuevo "espacio sem√°ntico", la posici√≥n de los vectores capture el significado de las palabras.
* Las palabras con significados similares estar√°n **cercanas** entre s√≠.
* Las relaciones entre palabras se pueden capturar con operaciones matem√°ticas.

**Analog√≠a:** Piensa en los colores. En lugar de un vector "one-hot" para cada color, podemos representarlos en el espacio RGB (3 dimensiones). En este espacio, "rojo" y "naranja" est√°n muy cerca, mientras que "rojo" y "azul" est√°n m√°s lejos. Los Word Embeddings hacen lo mismo, pero para el significado de las palabras.

---
### **3. El Nacimiento de los Embeddings Modernos: Word2Vec**

Un **Modelo de Lenguaje** es, en su forma m√°s b√°sica, un modelo que asigna una probabilidad a una secuencia de palabras. A menudo, esto se simplifica a la tarea de predecir la siguiente palabra dadas las palabras anteriores.

En 2013, un equipo de investigadores de Google (liderado por Tomas Mikolov) lanz√≥ **Word2Vec**, un modelo dise√±ado espec√≠ficamente para aprender embeddings de alta calidad de manera muy eficiente a partir de enormes cantidades de texto.

**La Hip√≥tesis Clave (Hip√≥tesis Distribucional):**
> "Sabr√°s lo que significa una palabra por la compa√±√≠a que mantiene."

Word2Vec no aprende el significado de una palabra de un diccionario. Aprende su significado observando las palabras que aparecen frecuentemente a su alrededor (su **contexto**).

**La Magia de las Relaciones Sem√°nticas:**
El espacio vectorial aprendido por Word2Vec era tan bueno que pod√≠a capturar relaciones complejas a trav√©s de la aritm√©tica vectorial. El ejemplo m√°s famoso es:

`vector('rey') - vector('hombre') + vector('mujer') ‚âà vector('reina')`

Esto demostr√≥ que los embeddings no eran solo listas de n√∫meros, sino que conten√≠an una rica estructura sem√°ntica.

---
### **4. La Limitaci√≥n Clave de los Embeddings Cl√°sicos**

A pesar de su poder, Word2Vec (y otros modelos similares como GloVe) tienen una limitaci√≥n fundamental: generan embeddings **est√°ticos** o **contexto-independientes**.

Cada palabra tiene un √∫nico vector asociado, sin importar c√≥mo se use en una oraci√≥n.

**El Problema del Contexto:**
Consideremos la palabra "**banco**".
1.  "Dej√≥ el dinero en el **banco**." (Instituci√≥n financiera)
2.  "Se sent√≥ en el **banco** del parque." (Asiento)

Para Word2Vec, el vector para "banco" es **exactamente el mismo** en ambas oraciones. El modelo es incapaz de capturar esta polisemia (m√∫ltiples significados).

Esta limitaci√≥n fue el principal motor para la siguiente gran revoluci√≥n en NLP. Se necesitaba una nueva generaci√≥n de modelos que pudieran generar embeddings **din√°micos** y **sensibles al contexto**, donde la representaci√≥n de una palabra cambiara seg√∫n la oraci√≥n en la que se encontrara.

Y esa es precisamente la puerta de entrada a las arquitecturas que veremos a continuaci√≥n, como el **Transformer**.