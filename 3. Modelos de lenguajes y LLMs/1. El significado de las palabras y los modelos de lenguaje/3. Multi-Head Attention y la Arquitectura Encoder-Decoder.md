## Multi-Head Attention y la Arquitectura Encoder-Decoder

**Objetivo de la Clase:**
* Entender el concepto de "Atención de Múltiples Cabezas" (Multi-Head Attention).
* Describir la arquitectura **Encoder-Decoder** completa del Transformer original.
* Diferenciar claramente la función del Encoder y del Decoder.
* Comprender los componentes clave del Decoder: la **Atención Enmascarada** y la **Atención Cruzada** (Cross-Attention).
* Comprender la necesidad y el funcionamiento de las "Codificaciones Posicionales" (Positional Encodings).

---
### **1. Multi-Head Attention: Múltiples Perspectivas del Texto**

La **Atención de Múltiples Cabezas** (Multi-Head Attention) es una evolución de la auto-atención. En lugar de tener un solo mecanismo de atención, tenemos un "comité de expertos" (múltiples "cabezas") que leen la misma oración en paralelo. Cada cabeza es libre de aprender y enfocarse en diferentes tipos de relaciones (sintácticas, semánticas, etc.), lo que permite capturar la riqueza del lenguaje de manera más robusta. Al final, las "opiniones" de todas las cabezas se combinan para formar una única y potente representación.

---
### **2. La Arquitectura Completa: El Modelo Encoder-Decoder**
El paper original, "Attention Is All You Need", propuso una arquitectura completa para tareas de **secuencia-a-secuencia (seq2seq)**, como la traducción automática. Esta arquitectura tiene dos partes principales: el Encoder y el Decoder.

![Arquietectura encoder-decoder](/imgs/Transformer_decoder.png)

#### **A. El Rol del Encoder (Codificador)**
El trabajo del Encoder es **"leer" y "entender"** la secuencia de entrada en su totalidad.
* Consiste en una pila de N bloques Encoder idénticos (el paper original usó 6).
* Cada bloque Encoder contiene las dos sub-capas que ya discutimos: **Multi-Head Attention** y una **Red Feed-Forward**. Ambas con sus conexiones residuales y normalización.
* La salida final del Encoder es un conjunto de vectores de contexto ricos en información (Keys y Values), uno por cada palabra de la secuencia de entrada. Esta salida representa el "significado" de la frase original.

#### **B. El Rol del Decoder (Decodificador)**
El trabajo del Decoder es **generar la secuencia de salida** (ej. la frase traducida), un token a la vez, utilizando la información del Encoder.
* También consiste en una pila de N bloques Decoder idénticos.
* Un bloque Decoder es similar a un bloque Encoder, pero tiene **tres sub-capas** en lugar de dos.

**Las Tres Sub-Capas del Bloque Decoder:**

1.  **Masked Multi-Head Self-Attention (Atención Propia Enmascarada):**
    * **Propósito:** Permite al Decoder mirar las palabras que ya ha generado en la secuencia de salida para contextualizar la siguiente predicción.
    * **La "Máscara":** Es la diferencia crucial. Al predecir la palabra en la posición `t`, esta atención está "enmascarada" para que **no pueda ver las palabras futuras** (en posiciones `t+1`, `t+2`, etc.). Esto es fundamental para que el modelo sea auto-regresivo y no "haga trampa" mirando la respuesta.

2.  **Encoder-Decoder Attention (Atención Cruzada o Cross-Attention):**
    * **¡Este es el puente entre el Encoder y el Decoder!**
    * **Propósito:** Permite que el Decoder "consulte" la representación de la frase de entrada que generó el Encoder.
    * **Mecanismo:** En este bloque de atención, los vectores **Query (Q)** provienen de la capa anterior del Decoder, pero los vectores **Key (K)** y **Value (V)** provienen de la **salida final del Encoder**.
    * Esto permite que cada palabra que se está generando en el Decoder preste atención a todas las palabras de la secuencia de *entrada*, para asegurarse de que la salida sea coherente con la entrada (ej. que la traducción sea correcta).

3.  **Red Feed-Forward:**
    * Idéntica a la del Encoder. Procesa la salida de la atención cruzada para refinar la representación.

Cada una de estas tres sub-capas también tiene su propia conexión residual y normalización de capa.

---
### **3. El Problema del Orden: Positional Encodings**
*(Esta sección también se mantiene igual, ya que es un concepto fundamental para toda la arquitectura)*

Como el Transformer procesa todas las palabras a la vez, no tiene una noción inherente del orden. Para solucionar esto, **sumamos** un vector de **Codificación Posicional** a cada embedding de palabra antes de que entre al modelo. Este vector le da al modelo una señal única sobre la posición (1ª, 2ª, 3ª, etc.) de cada palabra en la secuencia.

---
**Conclusión de la Arquitectura**

Al comprender esta estructura Encoder-Decoder, ahora podemos ver el panorama completo:
* La arquitectura completa es ideal para tareas **seq2seq** como traducción o resumen.
* Los modelos como **BERT** toman **solo la pila de Encoders** para especializarse en tareas de **comprensión** del lenguaje.
* Los modelos como **GPT** toman **solo la pila de Decoders** para especializarse en tareas de **generación** de lenguaje.

Esta visión más clara de la arquitectura completa nos da el contexto perfecto para entender por qué BERT y GPT están diseñados de la manera en que lo están.