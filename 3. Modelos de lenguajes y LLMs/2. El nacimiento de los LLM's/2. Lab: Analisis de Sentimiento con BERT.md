
## Análisis de Sentimientos con BERT y Keras

### **Objetivo del Laboratorio**
* Implementar un pipeline completo de fine-tuning para una tarea de Procesamiento del Lenguaje Natural (NLP).
* Utilizar la librería `transformers` de Hugging Face para cargar un modelo BERT pre-entrenado y su tokenizador.
* Preparar y tokenizar un dataset de texto (SST-2, Stanford Sentiment Treebank).
* Hacer fine-tuning del modelo BERT para clasificar el sentimiento de frases como "positivo" o "negativo".
* Evaluar el modelo y probarlo con nuevas frases.

### **Entorno**
Este laboratorio requiere Python, TensorFlow, y las librerías `transformers` y `tensorflow_datasets`. Se recomienda ejecutarlo en un entorno como Google Colab que proporciona acceso gratuito a GPUs.

---
### **Parte 0: Instalación y Configuración**
Primero, instalamos las librerías necesarias y las importamos.

```python
# Instalar las librerías de Hugging Face
!pip install transformers tensorflow-datasets -q

import tensorflow as tf
from tensorflow import keras
from transformers import BertTokenizer, TFBertForSequenceClassification
import tensorflow_datasets as tfds
import numpy as np
import matplotlib.pyplot as plt

print("Librerías importadas y listas.")
```

---
### **Parte 1: Carga y Exploración del Dataset (SST-2)**
Usaremos el dataset **SST-2 (Stanford Sentiment Treebank)**, una colección de frases extraídas de reseñas de películas, etiquetadas como sentimiento positivo (1) o negativo (0). Es un excelente dataset para una tarea de clasificación binaria.

```python
# Cargar el dataset SST-2 desde TensorFlow Datasets
(ds_train, ds_validation), ds_info = tfds.load(
    'glue/sst2',
    split=['train', 'validation'],
    with_info=True,
    as_supervised=True # Carga los datos como tuplas (entrada, etiqueta)
)

# Explorar algunos ejemplos
print("\nEjemplos del dataset:")
for sentence, label in ds_train.take(5):
    print(f"  Frase: {sentence.numpy().decode('utf-8')}")
    print(f"  Etiqueta: {'Positivo' if label.numpy() == 1 else 'Negativo'} ({label.numpy()})")
    print("-" * 20)

# Obtener el número de ejemplos
num_train_examples = ds_info.splits['train'].num_examples
print(f"\nNúmero de ejemplos de entrenamiento: {num_train_examples}")
```

---
### **Parte 2: Tokenización con el Tokenizador de BERT**
BERT no entiende texto crudo. Necesita que el texto se convierta en números (tokens) usando su propio vocabulario y reglas. Este proceso se llama **tokenización**.

* **[CLS]**: Token especial añadido al inicio. Su embedding de salida se usa para tareas de clasificación.
* **[SEP]**: Token especial para separar frases.
* **Attention Mask**: Le dice al modelo qué tokens son reales y cuáles son de "relleno" (padding).

```python
# Cargar el tokenizador de BERT
model_name = 'bert-base-uncased' # Usaremos el modelo base de BERT en minúsculas
tokenizer = BertTokenizer.from_pretrained(model_name)

# Parámetros
max_length = 128 # Longitud máxima de las secuencias
batch_size = 32

# Crear una función para tokenizar los datos
def tokenize_sentences(sentence, label):
    # El tokenizer de Hugging Face se encarga de todo:
    # añadir [CLS], [SEP], truncar, y crear la máscara de atención.
    inputs = tokenizer(
        sentence.numpy().decode('utf-8'), 
        add_special_tokens=True,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors="tf"
    )
    return {'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask']}, label

# Aplicar la tokenización al dataset usando .map()
# Esto es mucho más eficiente que un bucle for
ds_train_tokenized = ds_train.map(tokenize_sentences)
ds_validation_tokenized = ds_validation.map(tokenize_sentences)

# Preparar los datasets para el entrenamiento
ds_train_ready = ds_train_tokenized.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)
ds_validation_ready = ds_validation_tokenized.batch(batch_size).prefetch(tf.data.AUTOTUNE)

print("\nDatasets tokenizados y listos para el entrenamiento.")
```

---
### **Parte 3: Crear el Modelo de Fine-Tuning**
Cargaremos el modelo BERT pre-entrenado desde Hugging Face. `TFBertForSequenceClassification` ya incluye la base de BERT con una cabeza de clasificación encima, lista para ser fine-tuneada.

```python
# Cargar el modelo pre-entrenado
# num_labels=2 le dice al modelo que nuestra tarea es de clasificación binaria.
model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Compilar el modelo con una tasa de aprendizaje baja, como es estándar para fine-tuning
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]

model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

model.summary()
```

---
### **Parte 4: Entrenar (Fine-Tune) el Modelo**
Ahora, entrenamos el modelo. Con la potencia del Transfer Learning, no necesitamos muchas épocas para lograr un buen resultado.

```python
# Entrenar el modelo
epochs = 3
history = model.fit(
    ds_train_ready,
    validation_data=ds_validation_ready,
    epochs=epochs
)
```
---
### **Parte 5: Probar el Modelo en Nuevas Frases**
¡La parte divertida! Veamos cómo nuestro modelo clasifica frases que nunca ha visto.

```python
def predict_sentiment(text):
    """Toma una frase, la tokeniza y predice su sentimiento."""
    # Tokenizar la frase de entrada
    inputs = tokenizer(
        text, 
        add_special_tokens=True,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors="tf"
    )
    
    # Realizar la predicción
    outputs = model(inputs)
    logits = outputs.logits
    
    # Aplicar softmax para obtener probabilidades y predecir la clase
    probs = tf.nn.softmax(logits, axis=-1)
    prediction = tf.argmax(probs, axis=-1).numpy()[0]
    
    sentiment = "Positivo" if prediction == 1 else "Negativo"
    return sentiment, probs.numpy()[0]

# Frases de prueba
test_sentences = [
    "This movie was absolutely fantastic! I loved every second of it.",
    "A complete waste of time. The plot was predictable and boring.",
    "It was an okay movie, not great but not terrible either.",
    "The acting was brilliant, but the story was a bit weak."
]

print("\n--- Probando el modelo ---")
for sentence in test_sentences:
    sentiment, probs = predict_sentiment(sentence)
    print(f"\nFrase: '{sentence}'")
    print(f"Sentimiento Predicho: {sentiment}")
    print(f"Confianza -> Negativo: {probs[0]*100:.2f}%, Positivo: {probs[1]*100:.2f}%")
```

### **Conclusión y Discusión**
En este laboratorio, hemos implementado con éxito un pipeline de fine-tuning para BERT. Observa la alta precisión que se puede lograr con solo unas pocas épocas de entrenamiento. Esto demuestra el poder del Transfer Learning en NLP: en lugar de aprender el lenguaje desde cero, hemos "ajustado" el vasto conocimiento preexistente de BERT para nuestra tarea específica de análisis de sentimientos.