#### **Inferencia de Lenguaje Natural con BERT (Evaluación)**

##### **Descripción**
En esta evaluación, aplicarás la técnica de fine-tuning a un modelo Transformer pre-entrenado (BERT) para resolver una tarea de **Inferencia de Lenguaje Natural (NLI)**. A diferencia del análisis de sentimientos que procesa una sola frase, esta tarea requiere que el modelo analice un par de frases (una premisa y una hipótesis) y determine la relación lógica entre ellas. Utilizarás el dataset **RTE (Recognizing Textual Entailment)**, una de las subtareas del benchmark GLUE.

##### **Objetivo**
El objetivo principal es implementar un pipeline de NLP para la clasificación de pares de sentencias. Deberás demostrar tu habilidad para:
* Cargar y preprocesar un dataset que contiene pares de texto.
* Tokenizar correctamente las entradas de dos frases para un modelo tipo BERT, utilizando los tokens especiales apropiados.
* Hacer fine-tuning de un modelo `TFBertForSequenceClassification` para la tarea específica.
* Evaluar el rendimiento del modelo y utilizarlo para hacer predicciones sobre nuevos pares de frases.

##### **Instrucciones**

1.  **Carga y Preparación de Datos:**
    * Carga el dataset `glue/rte` desde la librería `tensorflow-datasets`. Asegúrate de cargar las divisiones de entrenamiento y validación.
    * Explora la estructura del dataset. Imprime algunos ejemplos para familiarizarte con las columnas (ej. `sentence1`, `sentence2`, `label`).
    * Identifica cuántas clases de etiquetas hay y qué representa cada una (ej. 0 = entailment, 1 = neutral, etc.).

2.  **Tokenización para Pares de Sentencias:**
    * Carga el tokenizador correspondiente al modelo `bert-base-uncased` desde la librería `transformers`.
    * Define una función que tome un ejemplo del dataset y lo convierta en un formato compatible con BERT. Esta función debe procesar ambas frases (`sentence1` y `sentence2`) juntas en una sola llamada al tokenizador.
    * Aplica esta función de tokenización a tus datasets de entrenamiento y validación usando el método `.map()`.
    * Prepara los datasets para el entrenamiento usando los métodos `.shuffle()`, `.batch()` y `.prefetch()`.

3.  **Creación y Compilación del Modelo:**
    * Carga el modelo `TFBertForSequenceClassification` desde `transformers`, pre-entrenado con los pesos de `bert-base-uncased`.
    * Asegúrate de configurar el modelo con el número correcto de etiquetas para la tarea RTE.
    * Compila el modelo. Deberás elegir un optimizador apropiado (Adam es una buena opción) con una tasa de aprendizaje baja adecuada para fine-tuning, y una función de pérdida para clasificación multiclase (como `SparseCategoricalCrossentropy`).

4.  **Entrenamiento del Modelo:**
    * Entrena el modelo en tu dataset de entrenamiento tokenizado durante un número adecuado de épocas (generalmente 3-5 épocas es suficiente para fine-tuning).
    * Utiliza el dataset de validación para monitorizar el rendimiento y detectar sobreajuste.

5.  **Evaluación y Prueba:**
    * Evalúa el rendimiento final de tu modelo en el conjunto de validación e informa de la precisión final.
    * Crea al menos 3 pares de frases (premisa/hipótesis) nuevos que no estén en el dataset. Intenta que cada par represente una de las posibles clases (entailment, neutral, contradiction).
    * Escribe una función que tome un par de frases como entrada, las procese, y utilice tu modelo fine-tuneado para predecir la relación lógica entre ellas. Presenta los resultados de tus 3 ejemplos.

##### **Sugerencias**

* Al cargar el dataset con `tfds.load`, usa `as_supervised=False` para poder acceder a las características por su nombre (ej. `ejemplo['sentence1']`).
* Recuerda que el tokenizador de Hugging Face puede manejar un par de frases directamente: `tokenizer(frase1, frase2, ...)`. Esto se encargará de añadir el token `[SEP]` entre ellas automáticamente.
* Para la compilación, una tasa de aprendizaje inicial de `3e-5` o `5e-5` suele ser un buen punto de partida para el fine-tuning de BERT.
* La salida del modelo (logits) no son probabilidades. Para interpretar la confianza de una predicción, deberás aplicar una función `softmax` a los logits.

---
## Laboratorio Práctico: Inferencia de Lenguaje Natural con BERT

### **Objetivo del Laboratorio**
* Implementar un pipeline de NLP para la clasificación de pares de sentencias usando BERT.
* Aprender a tokenizar y formatear entradas de dos frases con el token `[SEP]`.
* Hacer fine-tuning de un modelo BERT en el dataset RTE (Recognizing Textual Entailment).
* Evaluar y probar el modelo para predecir relaciones lógicas entre frases.

### **Entorno**
Este laboratorio requiere Python, TensorFlow, y las librerías `transformers` y `tensorflow_datasets`.

---
### **Parte 0: Instalación y Configuración**

```python
# Instalar las librerías de Hugging Face
!pip install transformers tensorflow tf-keras tensorflow-datasets -q

import os
import tensorflow as tf
from transformers import TFBertForSequenceClassification, AutoTokenizer
import tensorflow_datasets as tfds
import numpy as np
import matplotlib.pyplot as plt

print("Librerías importadas y listas.")


gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"✅ GPU disponible: {gpus[0].name}")
    except RuntimeError as e:
        print(f"❌ Error configurando la GPU: {e}")
else:
    print("⚠️ No se detectó GPU. Usando CPU.")

```
---
### **Parte 1: Carga y Exploración del Dataset (RTE)**
Cargaremos el dataset `glue/rte`, que es parte del benchmark General Language Understanding Evaluation.

```python
# Cargar el dataset RTE (Recognizing Textual Entailment)
(ds_train, ds_validation), ds_info = tfds.load(
    'glue/rte',
    split=['train', 'validation'],
    with_info=True,
    as_supervised=False # Cargamos como diccionario para acceder a las claves
)

# Explorar la información y algunos ejemplos
num_classes = ds_info.features['label'].num_classes
class_names = ds_info.features['label'].names
print(f"Número de clases: {num_classes}")
print(f"Nombres de las clases: {class_names}")

print("\nEjemplos del dataset:")
for example in ds_train.take(2):
    premise = example['sentence1'].numpy().decode('utf-8')
    hypothesis = example['sentence2'].numpy().decode('utf-8')
    label_index = example['label'].numpy()
    print(f"  Premisa: '{premise}'")
    print(f"  Hipótesis: '{hypothesis}'")
    print(f"  Etiqueta: {class_names[label_index]} ({label_index})")
    print("-" * 20)
```
---
### **Parte 2: Tokenización para Pares de Frases**
La clave aquí es pasar ambas frases al tokenizador. Él se encargará de formatearlo como `[CLS] premisa [SEP] hipótesis [SEP]`.

```python
# Cargar el tokenizador de BERT
model_name = 'bert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(model_name)

max_length = 256
batch_size = 16

def tokenize_sentence_pairs(example):
    def _tokenize(premise, hypothesis, label):
        premise = premise.numpy().decode('utf-8')
        hypothesis = hypothesis.numpy().decode('utf-8')

        inputs = tokenizer(
            premise,
            hypothesis,
            add_special_tokens=True,
            max_length=max_length,
            padding='max_length',
            truncation=True,
            return_tensors="tf"
        )

        input_ids = tf.squeeze(inputs['input_ids'], axis=0)         # Shape: [256]
        attention_mask = tf.squeeze(inputs['attention_mask'], axis=0)  # Shape: [256]

        return input_ids, attention_mask, label

    input_ids, attention_mask, label = tf.py_function(
        func=_tokenize,
        inp=[example['sentence1'], example['sentence2'], example['label']],
        Tout=[tf.int32, tf.int32, tf.int64]
    )

    input_ids.set_shape([max_length])
    attention_mask.set_shape([max_length])
    label.set_shape([])

    return {'input_ids': input_ids, 'attention_mask': attention_mask}, label

# Aplicar tokenización
ds_train_tokenized = ds_train.map(tokenize_sentence_pairs, num_parallel_calls=tf.data.AUTOTUNE)
ds_validation_tokenized = ds_validation.map(tokenize_sentence_pairs, num_parallel_calls=tf.data.AUTOTUNE)

# Preparar datasets
ds_train_ready = ds_train_tokenized.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)
ds_validation_ready = ds_validation_tokenized.batch(batch_size).prefetch(tf.data.AUTOTUNE)


print("\nDatasets tokenizados y listos para el entrenamiento.")
```
---
### **Parte 3: Crear, Compilar y Entrenar el Modelo**
El proceso es muy similar al de clasificación de una sola frase, pero el modelo aprenderá a usar la información de ambas frases gracias al formato de entrada.

```python
# Cargar el modelo pre-entrenado
model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)

# Compilar el modelo
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]

model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

# Entrenar el modelo
epochs = 4
history = model.fit(
    ds_train_ready,
    validation_data=ds_validation_ready,
    epochs=epochs
)
```
---
### **Parte 4: Probar el Modelo en Nuevas Frases**
Ahora, creemos una función para probar nuestro modelo "razonador" con nuevos pares de frases.

```python
def predict_nli(premise, hypothesis):
    """Toma un par de frases y predice su relación lógica."""
    inputs = tokenizer(
        premise,
        hypothesis,
        add_special_tokens=True,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors="tf"
    )
    
    outputs = model(inputs)
    logits = outputs.logits
    probs = tf.nn.softmax(logits, axis=-1)
    prediction_index = tf.argmax(probs, axis=-1).numpy()[0]
    prediction_label = class_names[prediction_index]
    
    return prediction_label, probs.numpy()[0]

plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title("Training vs Validation Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid()
plt.show()

# Pares de frases de prueba
test_pairs = [
    {"premise": "Un coche de carreras rojo está acelerando en la pista.", "hypothesis": "Un vehículo está en movimiento.", "expected": "entailment"},
    {"premise": "La cantante está en el escenario frente a miles de personas.", "hypothesis": "La habitación está vacía.", "expected": "contradiction"},
    {"premise": "El nuevo restaurante italiano abrió la semana pasada.", "hypothesis": "Sirven la mejor pizza de la ciudad.", "expected": "neutral"}
]

print("\n--- Probando el modelo NLI ---")
for pair in test_pairs:
    premise = pair["premise"]
    hypothesis = pair["hypothesis"]
    predicted_label, probs = predict_nli(premise, hypothesis)
    
    print(f"\nPremisa: '{premise}'")
    print(f"Hipótesis: '{hypothesis}'")
    print(f"-> Relación Predicha: {predicted_label} (Esperada: {pair['expected']})")
    # Imprimir la confianza en la predicción
    for i, label_name in enumerate(class_names):
        print(f"   Confianza en '{label_name}': {probs[i]*100:.2f}%")
```