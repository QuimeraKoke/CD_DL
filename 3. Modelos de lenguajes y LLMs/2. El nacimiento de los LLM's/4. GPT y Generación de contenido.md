## LLMs Basados en Decoders - El Arte de la Generación (GPT) ✍️

### **Introducción a la Familia GPT (Generative Pre-trained Transformer)**

**Objetivo de la Clase:**
* Entender la identidad arquitectónica de GPT como un modelo **decoder-only**.
* Comprender el concepto de **naturaleza auto-regresiva** (o unidireccional) y por qué es fundamental para la generación de texto.
* Identificar la tarea principal de GPT: predecir la siguiente palabra en una secuencia.

---

### **1. Una Arquitectura Diferente para una Tarea Diferente**

Anteriormente, nos enfocamos en BERT, un maestro de la **comprensión** del lenguaje. Su arquitectura *encoder-only* y su entrenamiento bidireccional lo hacen perfecto para analizar y extraer significado de un texto ya existente.

Pero, ¿qué pasa si nuestro objetivo no es entender texto, sino **crear texto nuevo** desde cero? Para esta tarea, necesitamos una filosofía y una arquitectura diferentes. Aquí es donde entra la familia de modelos **GPT (Generative Pre-trained Transformer)**.

**La Identidad de GPT: Un Modelo de Generación**
A diferencia de BERT, la arquitectura de GPT consiste únicamente en una pila de bloques **Decoder** de Transformer. Su diseño está optimizado para una única y poderosa tarea:

> Dado un fragmento de texto, predecir cuál será la siguiente palabra.

Todo el conocimiento y las capacidades de GPT (escribir poemas, código, resúmenes, etc.) se derivan de esta habilidad fundamental. GPT es un "escritor" experto, no un "lector" analítico como BERT.

---

### **2. La Naturaleza Auto-regresiva: Escribir Mirando Solo al Pasado**

La diferencia más importante entre GPT y BERT es la direccionalidad del contexto que utilizan.

* **BERT es bidireccional:** Para entender una palabra, mira todo lo que viene antes y después en la oración.
* **GPT es unidireccional o auto-regresivo:** Para predecir la siguiente palabra, **solo puede mirar las palabras que ya ha escrito** (el contexto a la izquierda). No puede ver el "futuro" de la oración.

**¿Por qué es esto esencial para la generación?**
Piénsalo como cuando escribes un correo electrónico. Para decidir qué palabra escribir a continuación, solo puedes basarte en las palabras que ya has tecleado. No sabes cómo terminará la oración antes de haber escrito el principio. GPT funciona de la misma manera, lo que le permite generar texto de forma coherente y secuencial, palabra por palabra.

Este comportamiento se logra gracias a la **Masked Self-Attention** (Atención Propia Enmascarada) dentro de los bloques Decoder, que estudiamos. Esta máscara oculta explícitamente los tokens futuros, forzando al modelo a ser auto-regresivo.


```
Ilustración del Flujo de Información:

BERT, para entender "pelota":
"El robot dejó caer la [pelota] porque él estaba dañado"


GPT, para predecir la siguiente palabra después de "pelota":
"El robot dejó caer la pelota [???]"
<-------------------------^
(Solo puede ver el contexto a la izquierda)
```

---

### **3. El Pre-entrenamiento de GPT: Un Objetivo Simple y Potente**

El pre-entrenamiento de GPT es conceptualmente más simple que el de BERT. No necesita tareas complejas como MLM o NSP. Su único objetivo es el **Modelado de Lenguaje Estándar (Standard Language Modeling)**.

* **La Tarea:** Al modelo se le presenta un fragmento de texto y se le entrena para predecir la siguiente palabra.
    * **Entrada:** "En un lugar de la Mancha, de cuyo nombre no quiero..."
    * **Objetivo:** Predecir la palabra "**acordarme**" con la mayor probabilidad posible.
* **El Proceso:** Este proceso se repite miles de millones de veces con todo el texto de internet. Al hacer esto, para convertirse en un buen predictor de la siguiente palabra, GPT se ve obligado a aprender de forma implícita:
    * Gramática y sintaxis.
    * Conocimiento del mundo (ej. "La capital de Francia es... París").
    * Estilos de escritura (formal, informal, poético, código).
    * Capacidad de razonamiento simple.

---

### **4. ¿Para Qué Sirve un Modelo Generativo?**

La simple habilidad de predecir la siguiente palabra se convierte en una potente capacidad de generación a través de un bucle:

1.  Se le da al modelo un texto inicial, llamado **prompt** (ej. "Érase una vez en un reino lejano,").
2.  El modelo predice la siguiente palabra más probable (ej. "una").
3.  Esta nueva palabra se añade al texto: "Érase una vez en un reino lejano, una".
4.  Esta secuencia más larga se vuelve a introducir en el modelo para predecir la siguiente palabra (ej. "princesa").
5.  Este proceso se repite hasta alcanzar una longitud deseada o un token de parada.

![Ejemplo del funcionamiento de un LLM para traducción](/imgs/transformer_decoding.gif)

Esto abre la puerta a una increíble variedad de aplicaciones, como:
* Generación de contenido creativo (historias, poemas).
* Chatbots y asistentes conversacionales.
* Resumen de textos.
* Escritura de código.
* Y muchas más.